#LyX 1.6.0rc3 created this file. For more info see http://www.lyx.org/
\lyxformat 340
\begin_document
\begin_header
\textclass latex8
\begin_preamble

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%


\usepackage{latex8}

\usepackage{times}

\usepackage{epsf}

\usepackage{epsfig}

\usepackage[config, font={sf,bf}]{caption}

\usepackage[config, font={small, sf}]{subfig}

\usepackage{latexsym}
   % additional packages that may be read in
    % to augment generic LaTeX; needed for \mathbb

\newcommand{\VD}{{Voronoi diagram}}
\newcommand{\VDs}{{Voronoi diagrams}}
%\documentstyle[times,art10,twocolumn,latex8]{article}

%-------------------------------------------------------------------------
% take the % away on next line to produce the final camera-ready version


%-------------------------------------------------------------------------
\end_preamble
\options times
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle empty
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Clusterers: a Comparison of Partitioning and Density-Based Algorithms and
 a Discussion of Optimisations
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
author{David Breitkreutz
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{
\backslash
& Information Technology} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia
\backslash

\backslash
 David.Breitkreutz@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
and
\end_layout

\begin_layout Plain Layout

Kate Casey
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit {
\backslash
& Information Technology} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Kate.Casey@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Though data mining is a relatively recent innovation, the improvements it
 offers over traditional data analysis have seen the field expand rapidly.
 Given the critical requirement for the efficient and accurate delivery
 of useful information in today's data-rich climate, significant research
 in the topic continues.
\end_layout

\begin_layout Abstract
Clustering is one of the fundamental techniques adopted by data mining tools
 across a range of applications.
 It provides several algorithms that can assess large data sets based on
 specific parameters and group related data points.
\end_layout

\begin_layout Abstract
This paper compares two widely used clustering algorithms, K-Medoids and
 DBSCAN, against other well-known techniques.
 The initial testing conducted on each technique utilises the standard implement
ation of each algorithm.
 Further experimental work tests modifications to these methods in order
 to investigate potential improvements of results or efficiency.
 Various key applications of clustering methods are detailed, and several
 areas of future work have been suggested.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Introduction"

\end_inset

Introduction 
\end_layout

\begin_layout Standard
Detail why clustering is powerful, why are we using it, what applications
 it applies to the most
\end_layout

\begin_layout Standard
Detail brief outline of clustering techniques (partitioning, density etc)
 and what they are most suited for (respectively)
\end_layout

\begin_layout Standard
Highlight what this paper's purpose is (to compare and contrast dbscan and
 k-medoids), and how it aims to achieve this (experimentation using WEKA
 to compare efficiency and accuracy of results)
\end_layout

\begin_layout Standard
Highlight that this paper will also attempt to suggest improvements to these
 algorithms (in order to try and obtain better results), and demonstrates
 its findings in this area
\end_layout

\begin_layout Standard
Lead into the related work section (eg many papers that already exist, and
 we talk about them now in Section 2)
\end_layout

\begin_layout Subsection
Importance
\end_layout

\begin_layout Standard
Clustering is promoted as an extremely powerful means of grouping related
 data points, and can efficiently reveal highly relevant trends in a source
 data set.
 These capabilities extend to large sets of data, and are applicable to
 stores of information used by scientists, researchers, and businesses.
 As a result, the field has developed into one of the foremost research
 areas in modern computing.
 These data mining techniques can analyse both large-scale databases and
 data warehouses.
 There exist numerous algorithms for performing such analysis, and each
 may be more suitable to certain circumstances, depending entirely upon
 domain-specific parameters.
 As such, the efficiency and accuracy of results from these data mining
 tasks relies directly upon the choice of a suitable algorithms.
 Thus, for as many different types of data set that exist, there is the
 requirement for further research into the improvements of these techniques.
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Data mining in general has a multitude of applications across a wide variety
 of fields.
 Pattern recognition for image analysis, medical diagnostics, sales forcasting
 and weather prediction are a recognised as a few of the more traditional
 usages.
 However, due to extensive development in the field and the recent explosion
 in data recording the capabilities extend far beyond these basic functions.
 Onboard computer analysis in vehicles, product quality analysis, targeted
 advertising campaigns, spam email filtration, fraud detection, and online
 crime analysis are but a few of the fields into which data mining now extends.
 
\begin_inset CommandInset citation
LatexCommand cite
key "john1999bsd"

\end_inset

 Clustering applies to all of the aforementioned applications as a subset
 of data mining.
 
\end_layout

\begin_layout Standard
A prominent example of this functionality is the application of emergency
 situation data analysis; for example, data recorded by authorities during
 forest fires.
 This paper will investigate that specific scenario in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Details"

\end_inset

, with the test sets utilised containing fire data from northeastern Portugal
\emph on
.
\end_layout

\begin_layout Subsection
Other Details
\end_layout

\begin_layout Standard
There are a wide variety of clustering methodologies that exist within this
 field.
 These include, but are not limited to, grid-based categorisation, density-based
 grouping, hierarchical segmentation, and constraint-based analysis.
 As these techniques have been already widely researched, there are many
 existing works that compare and constrast each.
 
\end_layout

\begin_layout Standard

\emph on
[NEED REFS HERE]
\end_layout

\begin_layout Standard
The research outlined in this paper is concerned with the application of
 two of the most widely used methods; 
\emph on
partitioning-based 
\emph default
and 
\emph on
density-based
\emph default
 clustering.
\end_layout

\begin_layout Standard
Partitioning algorithms are effective for mining data sets when computation
 of a dendrogram (clustering tree) representation is infeasible.
 
\begin_inset CommandInset citation
LatexCommand cite
key "jain1999dcr"

\end_inset

 In particular, K-Medoids is highly efficient for moderately sized data
 sets with spherical-type clusters.
 However, due to the inherent swapping of medoids to optimise the clustered
 solution, the algorithm suffers greatly as large data sets are introduced.
 
\end_layout

\begin_layout Standard
Density-based algorithms perform optimally when operating upon spatially-indexed
 data.
 The methods provide benefits when analysing data sets that contain high
 levels of noise or when clusters are arbitrarily shaped.
 Specifically, DBSCAN is able to grow clusters whilst the MinPts threshold
 is not satisfied within the a specified neighbourhood, thus efficiently
 dividing real data and noise in a variety of shapes.
\end_layout

\begin_layout Standard
In terms of algorithms, this paper focuses upon the DBSCAN and K-Medoids
 algorithms from the density-based and partitioning families respectively,
 and initiates new research into improving the efficiency of each.
\end_layout

\begin_layout Standard
The data set chosen for the comparison of and ammendments to K-medoids and
 DBSCAN is 
\emph on
forest fires
\emph default
.
 
\end_layout

\begin_layout Standard

\emph on
[Describe what the specific data set we've chosen here, and why our selected
 algorithm(s) are worthwhile to be 
\emph default
executed
\emph on
 here.
 Why is the data set relevant to some aspect of life on earth (and why should
 we care)? - this makes the problem important to tackle, and should make
 people interested]
\end_layout

\begin_layout Standard
This paper proposes both implementations and improvements to the K-Medoids
 and DBSCAN clustering algorithms.
 A great deal of literature already exists regarding these techniques, and
 a review of these documents can be found in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
[Brief introduction as to the results of the comparison were..
 what the results indicate (overview) The improvements this paper suggests
 to both the DBSCAN and K-medoids algorithms show a level of improvement
 for the data set that is utilised for testing.
 This suggests potential for further improvement given additional levels
 of interest....]
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Related-Work"

\end_inset

Related Work
\end_layout

\begin_layout Standard
Detail the specific areas that we are looking at (density and partitioning),
 and what each is most useful for (shape of data, noise, features etc)
\end_layout

\begin_layout Standard
Provide a paragraph of two for hierarchical etc techniques that the we don't
 cover, and direct the user elsewhere
\end_layout

\begin_layout Standard
Detail a section for alternative data analysis methods to clustering (and
 why they might be useful elsewhere, but not here)
\end_layout

\begin_layout Subsection
Data Analysis
\end_layout

\begin_layout Standard
The process of data analysis involves considering all elements within a
 given source data set in order to deduce interesting patterns and trends.
 
\end_layout

\begin_layout Standard
What is data analysis first (used to obtain patterns and trends from a source
 set of data, with the information used to improve the decision making process
 (people can be more informed)
\end_layout

\begin_layout Subsection
Clustering
\end_layout

\begin_layout Standard
Detail what is clustering (own heading for this)
\end_layout

\begin_layout Standard
Aggarwal et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "aggarwal1999fap"

\end_inset

 define the process of clustering as:
\emph on
 
\begin_inset Quotes eld
\end_inset

Given a set of points in multidimensional space, find a partition of the
 points into clusters so that the points within each cluster are close to
 one another
\begin_inset Quotes erd
\end_inset


\emph default
.
 Proximity is measured using a variety of algorithm-specific metrics, such
 that the closer two arbitrary points are to one another, the more strongly
 they are considered to be related.
 This process results in defined groupings of similar points, where strong
 inter-cluster and weak intra-cluster relationships exist between points,
 and example of which is detailed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-clustered"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename clustering.png
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-clustered"

\end_inset

Example of clustered points
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clustering can be categorised in machine learning terms as a form of 
\emph on
unsupervised learning
\emph default
; that is, clusters are representative of hidden patterns in the source
 data set
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

.
 Raw information is analysed and relationships are discovered by the algorithm
 without external direction or interference; learning through observation
 rather than by the study of examples
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 In addition, this objectivity translates to an effective means of data
 analysis without the opportunity for subjective human conclusions to be
 drawn from data.
\end_layout

\begin_layout Subsection
Partitioning Clustering Algorithms
\end_layout

\begin_layout Standard
Partitioning methods define clusters by grouping data points into 
\emph on
k
\emph default
 partitions (defined at runtime).
 A point is determined to be 
\emph on
similar
\emph default
 to other points within its partition, and 
\emph on
dissimilar
\emph default
 to points that lie outside the boundary of that partition.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 Comparison is based on the characteristics of the data set provided.
 Thus, the algorithms rely on the conversion of semantic data attributes
 (width, height, shape, colour, cost etc.) into points that determine physical
 location on a set of mathematical axes.
 This provides an objective and computationally acceptable framework for
 analysis.
 In the simplest case only two attributes exist, and thus the conversion
 renders a point on a standard Cartesian plane.
 This process is greatly complicated when, as often occurs in highly detailed
 source sets, hundreds of attributes are present.
 The rendering plane takes on high dimensionality, and the complexity of
 analysis becomes very computationally expensive.
 
\end_layout

\begin_layout Standard
This paper studies the K-Medoids algorithm from the partitioning analysis
 family.
 The technique was first developed in 1987 by Kaufman & Rousseeuw and seeks
 to reduce the impact of noisy data experienced in more simple partitioning
 algorithms.
 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1987cmm"

\end_inset

 Instead of converting data attributes to simple points, it retains all
 the information from each record and represents the points as objects on
 a detailed axes.
 It initially chooses 
\emph on
k
\emph default
 arbitrary points as representative medoids (
\emph on
O
\begin_inset Formula $\mathcal{_{\text{r}}}$
\end_inset


\emph default
), then iteratively compares each chosen point with every other point (O
\begin_inset Formula $_{\text{j}}$
\end_inset

) to determine if swapping the medoid is useful.
 This decision is based on a cost function; the average value of some dissimilar
ity metric between any object and the medoid of its cluster.
 
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

 The algorithm terminates when every point has been assigned to a cluster
 and swapping medoids and objects no longer has any optimising effect on
 the clustering outcome.
\end_layout

\begin_layout Standard
Partitioning offers several features, including efficiency in processing
 spherical clusters in small to medium data sets, and scalability across
 smaller applications.
 However, the algorithms suffer from extreme complexity and computational
 cost at higher dimensions in large sets, require the user to specify the
 number of clusters to be determined (
\emph on
k
\emph default
) and experience problems defining arbitrarily shaped clusters.
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Subsection
Density-Based Clustering Algorithms
\end_layout

\begin_layout Standard
The density-based group of clustering algorithms represent a data set in
 the same manner as partitioning methods; converting an instance to a point
 using the data attributes of the source set.
 The plane contains clusters with high internal density and low external
 density in a similar manner to its partitioning ancestor.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Kolatch01clusteringalgorithms"

\end_inset

 As a result, analysis can easily isolate noise instances from relevant
 instances.
 
\end_layout

\begin_layout Standard
This paper studies the DBSCAN algorithm from the density-based algorithm
 collection, first proposed in 1996 by Ester et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

 The technique relies upon an epsilon value (
\begin_inset Formula $\mathcal{\varepsilon}$
\end_inset

 - defined as the radius of the neighbourhood of a point based on some distance
 metric), and a minimum points value (
\emph on
MinPts
\emph default
 - defined as the minimum number of points required to exist in a neighbourhood
 to be declared a cluster).
 The method examines the 
\begin_inset Formula $\varepsilon$
\end_inset

-neighbourhood of every point in the set, and if the region contains more
 points than the MinPts threshold a new cluster is defined.
 The examined point is declared the centre point, and the algorithm then
 iteratively collects surrounding density-reachable objects and attaches
 them to this new cluster.
 Such behaviour may trigger cluster merging depending upon point proximity
 and, of course, the runtime parameters 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\varepsilon$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 and MinPts.
 The algorithm terminates when all defined clusters are finalised; no further
 points can be added to any cluster and any unclustered points are declared
 as 
\emph on
noise
\emph default
.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 Due to this method of analysis, DBSCAN is designed to analyse data sets
 with arbitrary shapes, and can cluster sets that include hollow formations.
 
\end_layout

\begin_layout Standard
Density-based algorithms provide advantages over other methods through their
 noise handling capabilities and ability to determine clusters with arbitrary
 shapes (eg.
 a circle, torus or other non-convex formation).
 As with partitioning techniques, computational cost is a disadvantage when
 the technique is used with large amounts of source data and sets containing
 excessive noise.
 
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Subsection
Other Techniques
\end_layout

\begin_layout Standard
Clustering differs from other classical data mining methodologies in a variety
 of ways and extensive listerature already exists explaining this.
 For the purpose of conciseness, this paper will compare and contrast clustering
 only with the techniques of 
\emph on
classification
\emph default
 and 
\emph on
prediction
\emph default
.
\end_layout

\begin_layout Standard
Classification is the process of constructing a learning model, or 
\emph on
classifier
\emph default
, that assigns points in a dataset to predefined classes
\begin_inset CommandInset citation
LatexCommand cite
key "fayyad1996dma"

\end_inset

.
 Whilst this definition initially appears similar to that of clustering
 in that points are grouped based on attributes, any similarities are thus
 exhausted.
 The technique utilises 
\emph on
supervised learning 
\emph default
as opposed to the unsupervised learning environment of clustering.
 The model is compiled by studying provided data (known as a training set)
 containing class labels; hence the technique is 
\begin_inset Quotes eld
\end_inset

supervised
\begin_inset Quotes erd
\end_inset

 due to example data exposure.
 Once the model is complete it can be applied to real, unlabelled data and
 assign each new tuple to its respective class
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 Classification is used in the categorisation of discrete or unordered data
 sets; for example, determining the level of risk (high/medium/low) associated
 with a certain financial investment given pre-existing market data.
 
\end_layout

\begin_layout Standard
Prediction is similar to classification, in that the process of learning
 from existing data is supervised, and a model is constructed.
 However, prediction techniques are used when assessing continuous data
 sets 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

; for example, determining the price of a stock market share at some point
 in the future given previous performance trends.
 Unlike prediction, clustering is not exclusively used for forcasting future
 values, rather grouping like points for immediate assessment.
\end_layout

\begin_layout Standard
Whether to utilise clustering in analysis is a decision based on the purpose
 of the data mining task.
 If future predictions are required, an alternative technique should be
 employed, but if like points in the set are to be grouped, clustering is
 an appropriate choice.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Comparison"

\end_inset

Comparison
\end_layout

\begin_layout Standard
Comparing the specific techniques themselves (so we've detailed what the
 types of clustering are in section 2, here we talk about what the technique
 of DBSCAN etc does
\end_layout

\begin_layout Standard
The implementation for both algorithms has been designed for and completed
 in Java and all operations have been tested under the WEKA data mining
 framework.
 
\end_layout

\begin_layout Standard
Need to run a comparison (on our sample data set) of our two algorithms
 we've implemented, and several others that already exist within WEKA.
 How does their 1) 
\emph on
performance
\emph default
 and 2) 
\emph on
accuracy
\emph default
 stand up to one another? May need to look at how many clusters (on average)
 each method produces and them perform the same function like this (keep
 running until completed)
\end_layout

\begin_layout Standard
big O stuff??.
\end_layout

\begin_layout Standard
will write algorithms out
\end_layout

\begin_layout Subsection
DBSCAN
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
DBSCAN
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
For each unvisited point 
\begin_inset Formula $P$
\end_inset

 within the data set 
\begin_inset Formula $D$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Get the neighbours of the 
\begin_inset Formula $P$
\end_inset

, according to the given epsilon distance
\end_layout

\begin_layout Enumerate
If the number of neighbours is equal or greater than the user-specified
 cluster threshold
\end_layout

\begin_deeper
\begin_layout Enumerate
Increment cluster identifier counter
\end_layout

\begin_layout Enumerate
Add the given point to the current cluster
\end_layout

\begin_layout Enumerate
Recursively process all neighbours 
\end_layout

\end_deeper
\begin_layout Enumerate
Else if the point has fewer neighbours than the threshold value,
\end_layout

\begin_deeper
\begin_layout Enumerate
Mark 
\begin_inset Formula $P$
\end_inset

 as noise
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Subsection
OPTICS
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
OPTICS
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily select 
\begin_inset Formula $k$
\end_inset

 objects as initial medoid points from given data set
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
K-Medoids
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
K-Medoids
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily select 
\begin_inset Formula $k$
\end_inset

 objects as initial medoid points from given data set
\end_layout

\begin_layout Enumerate
Whilst swaps are occuring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data object in the data set with its closest medoid
\end_layout

\begin_layout Enumerate
Randomly select a non-medoid object, 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
Compute the total cost of swapping initial medoid object with 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
If the total cost of swapping is less than the total cost of the current
 system then
\end_layout

\begin_deeper
\begin_layout Enumerate
Swap the inital medoid with 
\begin_inset Formula $O$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Continue until there is no change
\end_layout

\end_deeper
\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
K-Means
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
K-Means
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily or heuristically select 
\begin_inset Formula $k$
\end_inset

 objects as initial centroids for the given data set
\end_layout

\begin_layout Enumerate
Whilst centroid changes are occuring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data point 
\begin_inset Formula $P$
\end_inset

 in the data set to its closest centroid
\end_layout

\begin_layout Enumerate
For each centroid 
\begin_inset Formula $C$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Recompute location according to data points associated with it
\end_layout

\end_deeper
\begin_layout Enumerate
Continue until stopping criteria met
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Section
Customisations
\end_layout

\begin_layout Standard
Detail here what we did to modify the algorithms that we implemented
\end_layout

\begin_layout Subsection
UltraDBSCAN
\end_layout

\begin_layout Standard
Dynamically modify the value of epsilon such that it becomes adjusted according
 to how many points are present within the current cluster.
 This modification was considered for use within data sets that a constant
 distance value would be ineffective.
 By reducing the epsilon value according to current cluster size, the clusters
 found aim to be denser, and the potential for the 'bridging effect' is
 attempted to be minimised.
 The consideration here is that a very dense cluster will see its members
 similarly spaced, and for any given epislon value, if many clusters are
 found under such a value, then epsilon can be decreased to reduce the effect
 of noise.
 See Figure X for details.
\end_layout

\begin_layout Subsection
UltraK-Medoids
\end_layout

\begin_layout Standard
Only recalculate the medoid associations of points once per swap iteration.
 At present, the algorithm re-calculates the medoid assignments for all
 points in order to determine the total cost of swapping the system.
 In many situations, this would be grossly inefficient as most points should
 remain associated with the same medoid.
 The only data instances that would change are those which have close to
 dual-medoid membership, given the location of the new, potential medoid.
 Thus, the system instead only swaps the medoid out temporarily with the
 other randomly selected medoid, and then computes the total cost without
 recomputing all distances.
 
\end_layout

\begin_layout Standard
The result is that the time complexity reduces from approximately 
\begin_inset Formula $O(2$
\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Details"

\end_inset

Experimental Details
\end_layout

\begin_layout Standard
Detail how the experiements were carried out, (using WEKA, using the given
 data set of forest fires, and reference this)
\end_layout

\begin_layout Standard
Mention the computiing hardware it was run upon (one sentance or so), implemente
d in Java, uses the algorithms detailed in both Section 3 and section 4,
 and compares and contrasts them.
\end_layout

\begin_layout Standard
Testing performed with varying input parameters to determine their effect
 on the results, and to see if the customisations offer any improvement
 in speed or results produced over their original counterparts
\end_layout

\begin_layout Subsection
Comparison Results
\end_layout

\begin_layout Standard
Discuss what the results were here, so detail the graphs and tables for
 efficiency and numbers etc.
\end_layout

\begin_layout Subsection
Conclusions
\end_layout

\begin_layout Standard
As we detail the results, we should offer a discussion as to what they are,
 and why they are like that.
\end_layout

\begin_layout Standard
Eg draw conclusions here and offer explanations
\end_layout

\begin_layout Section
Issues
\end_layout

\begin_layout Standard
Discuss the results in this section
\end_layout

\begin_layout Standard
Is there sufficient evidence here to show that our algorithms are worthy
 of use?
\end_layout

\begin_layout Standard
Issues - What drawbacks do we see with our various sets of data and algorithms
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Conclusion"

\end_inset

Conclusion
\end_layout

\begin_layout Standard
Overall: better or worse? Able to satisfy or not-possible? It might not
 be given the complexity and time limitations
\end_layout

\begin_layout Standard
summarise topic and close.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Future-Work"

\end_inset

Future Work
\end_layout

\begin_layout Standard
Describe potential improvements to the methodologies we've presented within
 this paper.
 Others can extend our work by seeking other ways of improving the semantics
 of the algorithm, rather than attempting to improve its operation itself.
 Given that these algorithms this paper discusses have been in existence
 for <X> years, and that there have been many various papers and theses
 produced regarding these issues, this appears the logical path for improvements
 to take.
 Should any future developers decide to follow through on the same path,
 this paper has logically set out its concepts and ideas, and will provide
 a solid basis for such research.
\end_layout

\begin_layout Standard
As the topic of data mining, and more specifically, clustering, becomes
 more of a part of every-day business and organisational operation, the
 necessity for faster and equally accurate algorithms rises.
 This paper has shown this, and several other applications do exist, and
 it can only be assumed that 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "clustersdk"
options "latex8"

\end_inset


\end_layout

\end_body
\end_document
