#LyX 1.6.0rc3 created this file. For more info see http://www.lyx.org/
\lyxformat 340
\begin_document
\begin_header
\textclass latex8
\begin_preamble

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%


\usepackage{latex8}

\usepackage{times}

\usepackage{epsf}

\usepackage{epsfig}

\usepackage[config, font={sf,bf}]{caption}

\usepackage[config, font={small, sf}]{subfig}

\usepackage{latexsym}
   % additional packages that may be read in
    % to augment generic LaTeX; needed for \mathbb

\newcommand{\VD}{{Voronoi diagram}}
\newcommand{\VDs}{{Voronoi diagrams}}
%\documentstyle[times,art10,twocolumn,latex8]{article}

%-------------------------------------------------------------------------
% take the % away on next line to produce the final camera-ready version


%-------------------------------------------------------------------------
\end_preamble
\options times
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle empty
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Clusterers: A Comparison of Partitioning and Distance-Based Algorithms and
 A Discussion of Optimisations
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
author{David Breitkreutz
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics 
\backslash
& Information Technology } 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia
\backslash

\backslash
 David.Breitkreutz@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
and
\end_layout

\begin_layout Plain Layout

Kate Casey
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics 
\backslash
& Information Technology} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Kate.Casey@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Though data mining is a relatively recent innovation, the improvements it
 offers over traditional data analysis have seen the field expand rapidly.
 Given the critical requirement for the efficient and accurate delivery
 of useful information in today's data-rich climate, significant research
 in the topic continues.
\end_layout

\begin_layout Abstract
Clustering is one of the fundamental techniques adopted by data mining tools
 across a range of applications.
 It provides several algorithms that can assess large data sets based on
 specific parameters and group related data points.
\end_layout

\begin_layout Abstract
This paper presents detailed comparison of two widely used clustering algorithms
, K-Medoids and DBSCAN, against others.
 The initial testing conducted on each technique utilises the standard implement
ation of the algorithm.
 Further experimental work goes on to test hybrid versions of the methods
 for potential efficiency enhancements.
 Various key applications of clustering methods are detailed, and several
 areas of future work have been suggested.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Introduction"

\end_inset

Introduction 
\end_layout

\begin_layout Standard
Aggarwal et al define the process of clustering as: 
\begin_inset Quotes eld
\end_inset

Given a set of points in multidimensional space, find a partition of the
 points into 
\emph on
clusters
\emph default
 so that the points within each cluster are close to one another
\begin_inset Quotes erd
\end_inset

.
\begin_inset CommandInset citation
LatexCommand cite
key "aggarwal1999fap"

\end_inset

 Proximity is measured using a variety of algorithm-specific metrics, and
 the 
\begin_inset Quotes eld
\end_inset

closer
\begin_inset Quotes erd
\end_inset

 two arbitrary points are to one another, the more strongly they are related.
 This process results in defined groupings of similar points, where strong
 inter-cluster and weak intra-cluster relationships exist between points.
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Standard
Clustering can be categorised in machine learning terms as a form of 
\emph on
unsupervised learning
\emph default
; that is, clusters are representative of hidden patterns in the source
 data set.
 
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

 Raw information is analysed and relationships are discovered by the algorithm
 without external direction or interference; learning through observation
 rather than by the study of examples.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 In addition, this objectivity translates to an effective means of data
 analysis without the opportunity for subjective human conclusions to be
 drawn from data.
 
\end_layout

\begin_layout Standard
Clustering is promoted as an extremely powerful means of grouping related
 data points and efficiently revealing highly relevant trends in the source
 set.
 These capabilities extend to very large data sets, and are applicable to
 the masses of information stored by scientists, medicos and businesses
 (to name a few) every day.
 As a result, the field has developed into one of the foremost research
 areas in modern computing given the broad scope for such techniques in
 analysing large-scale databases and data warehouses.
 It already provides numerous algorithms that can perform such analysis
 based on a series of domain-specific parameters.
 These tasks are performed with varying degrees of efficiency depending
 upon the specific algorithm, and the characteristics of the data set it
 operates upon.
 Thus, there is clear grounds for further research into improving the performanc
e of such algorithms.
 
\end_layout

\begin_layout Standard
Several clustering methodologies exist including heirarchical, partitioning,
 density-based (subset of partitioning), constraint-based and grid-based
 algorithms.
 A great deal of literature already exists in each of these fields, and
 while analysis of each is a fascinating, it has been covered more than
 adequately by numerous authors.
 This paper focusses only on the density-based and partitioning sectors,
 specifically the DBSCAN and K-Medoids algorithms respectively, and initiates
 new research into improving the efficiency of each.
\end_layout

\begin_layout Subsection
Partitioning Algorithms
\end_layout

\begin_layout Standard
Partitioning methods define clusters by grouping data points into 
\emph on
k
\emph default
 partitions (defined at runtime).
 A point is determined to be 
\emph on
similar
\emph default
 to other points within its partition, and 
\emph on
dissimilar
\emph default
 to points that lie outside the boundary of that partition.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 Comparison is based on the characteristics of the data set provided.
 Thus, the algorithms rely on the conversion of semantic data attributes
 (width, height, shape, colour, cost etc.) into points that determine physical
 location on a set of mathematical axes.
 This provides an objective and computationally acceptable framework for
 analysis.
 In the simplest case only two attributes exist, and thus the conversion
 renders a point on a standard Cartesian plane.
 This process is greatly complicated when, as often occurs in highly detailed
 source sets, hundreds of attributes are present.
 The rendering plane takes on high dimensionality, and the complexity of
 analysis becomes very computationally expensive.
 
\end_layout

\begin_layout Standard
This paper studies the K-Medoids algorithm from the partitioning analysis
 family.
 The method seeks to reduce the impact of noisy data experienced in more
 simple partitioning algorithms.
 Instead of converting data attributes to simple points, it retains all
 the information from each record and represents the points as objects on
 a detailed axes.
 It initally chooses 
\emph on
k
\emph default
 arbitrary points as representative medoids (
\emph on
O
\begin_inset Formula $\mathcal{_{\text{r}}}$
\end_inset


\emph default
), then iteratively compares each chosen point with every other point (O
\begin_inset Formula $_{\text{j}}$
\end_inset

) to determine if swapping the medoid is useful.
 This decision is based on a cost function; the average value of some dissimilar
ity metric between any object and the medoid of its cluster.
 
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

 The algorithm terminates when every point has been assigned to a cluster
 and swapping medoids and objects no longer has any optimising effect on
 the clustering outcome.
\end_layout

\begin_layout Standard
[positives and negatives]
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Subsection
Density-Based Algorithms
\end_layout

\begin_layout Standard
The density-based group of algorithms represent a data set in the same manner
 as partitioning methods; converting an instance to a point using the data
 attributes of the source set.
 The plane contains clusters with high internal density and low external
 density in a similar manner to its partitioning ancestor.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Kolatch01clusteringalgorithms"

\end_inset

  As a result, analysis can easily isolate noise instances from relevant
 instances.
 
\end_layout

\begin_layout Standard
This paper studies the DBSCAN algorithm from the density-based algorithm
 collection.
 The technique relies upon an epsilon [
\begin_inset Formula $\varepsilon$
\end_inset

] value (defined as the radius of the neighbourhood of a point based on
 some distance metric), and a minimum points [MinPts] value (defined as
 the minimum number of points required to exist in a neighbourhood to be
 declared a cluster).
 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

 The method examines the 
\begin_inset Formula $\varepsilon$
\end_inset

-neighbourhood of every point in the set, and if the region contains more
 points than the MinPts threshold a new cluster is defined.
 The examined point is declared the centre point, and the algorithm then
 iteratively collects surrounding density-reachable objects and attaches
 them to this new cluster.
 Such behaviour may trigger cluster merging depending upon point proximity
 and, of course, the runtime parameters 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\varepsilon$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 and MinPts.
 The algorithm terminates when all defined clusters are finalised; no further
 points can be added to any cluster and any unclustered points are declared
 as 
\emph on
noise
\emph default
.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 Due to this method of analysis, DBSCAN is designed to analyse data sets
 with arbitrary shapes, and can cluster sets that include hollow formations.
 
\end_layout

\begin_layout Standard
[positives and negatives]
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Data mining in general has a multitude of applications across a wide variety
 of fields.
 Pattern recognition for image analysis, medical diagnostics, sales forcasting
 and weather prediction are a recognised as a few of the more traditional
 usages.
 However, due to extensive development in the field and the recent explosion
 in data recording the capabilities extend far beyond these basic functions.
 Onboard computer analysis in vehicles, product quality analysis, targeted
 advertising campaigns, spam email filtration, fraud detection, and online
 crime analysis are but a few of the fields into which data mining now extends.
 
\begin_inset CommandInset citation
LatexCommand cite
key "john1999bsd"

\end_inset


\end_layout

\begin_layout Standard
Clustering applies to all of the aforementioned applications as a subset
 of data mining.
 Partitioning techniques are effective for mining data sets when computation
 of a dendrogram (clustering tree) representation is infeasable.
 
\begin_inset CommandInset citation
LatexCommand cite
key "jain1999dcr"

\end_inset

 In pariticular, K-Medoids is highly efficient for small data sets.
 However, due to the inherent swapping of medoids to optimise the clustered
 solution, the algorithm suffers greatly as large data sets are introduced.
 Density-based algorithms provide benefits when analysing data sets that
 contain high levels of noise or when clusters are arbitrarily shaped (eg.
 a torus or other non-convex formation).
\end_layout

\begin_layout Subsection
Experimental Parameters
\end_layout

\begin_layout Standard
Describe what the specific data set we've chosen here, and why our selected
 algorithm(s) are worthwhile to be 
\emph on
executed
\emph default
 here
\end_layout

\begin_layout Standard
implementation in WEKA
\end_layout

\begin_layout Standard
Why is the data set relevant to some aspect of life on earth (and why should
 we care)? - this makes the problem important to tackle, and should make
 people interested
\end_layout

\begin_layout Standard
what's being proposed in the paper + related references
\end_layout

\begin_layout Standard
Brief introduction as to the results of the comparison were..
 what the results indicate (overview) The improvements this paper suggests
 to both the DBSCAN and K-medoids algorithms show a level of improvement
 for the data set that is utilised for testing.
 This suggests potential for further improvement given additional levels
 of interest....
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Related-Work"

\end_inset

Related Work
\end_layout

\begin_layout Standard
Detail about the algorithms themselves, who introduced them (what's been
 done on them before)
\end_layout

\begin_layout Standard
the standard methods + drawbacks
\end_layout

\begin_layout Standard
alternatives/other improvements suggested
\end_layout

\begin_layout Standard
Literature review of related works (highlight past examples of papers) that
 have done similar comparisons before
\end_layout

\begin_layout Standard
What is clustering?
\end_layout

\begin_layout Standard
How close is it to things like association or classification?
\end_layout

\begin_layout Standard
Why use clustering over these others?
\end_layout

\begin_layout Standard
Algorithms: what do we have available, who developed them originally, improvemen
ts?
\end_layout

\begin_layout Standard
Usefulness and detriments of each algorithm, best types of data set (if
 any)?
\end_layout

\begin_layout Standard
Generalised overview - who's done surveys like this before, what did they
 find out, what we set out to do better than them
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Comparison"

\end_inset

Comparison
\end_layout

\begin_layout Standard
Need to run a comparison (on our sample data set) of our two algorithms
 we've implemented, and several others that already exist within WEKA.
 How does their 1) 
\emph on
performance
\emph default
 and 2) 
\emph on
accuracy
\emph default
 stand up to one another? May need to look at how many clusters (on average)
 each method produces and them perform the same function like this (keep
 running until completed)
\end_layout

\begin_layout Standard
big O stuff??.
\end_layout

\begin_layout Standard
need diagrams
\end_layout

\begin_layout Subsection
DBSCAN
\end_layout

\begin_layout Subsection
K-Medoids
\end_layout

\begin_layout Subsection
K-Means etc
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Discussion"

\end_inset

Discussion
\end_layout

\begin_layout Standard
Is there sufficient evidence here to show that our algorithms are worthy
 of use?
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Issues"

\end_inset

Issues
\end_layout

\begin_layout Standard
What drawbacks do we see with our various sets of data and algorithms
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Conclusion"

\end_inset

Conclusion
\end_layout

\begin_layout Standard
Overall: better or worse? Able to satisfy or not-possible? It might not
 be given the complexity and time limitations
\end_layout

\begin_layout Standard
summarise topic and close.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Future-Work"

\end_inset

Future Work
\end_layout

\begin_layout Standard
Describe potential improvements to the methodologies we've presented within
 this paper.
 Others can extend our work by seeking other ways of improving the semantics
 of the algorithm, rather than attempting to improve its operation itself.
 Given that these algorithms this paper discusses have been in existence
 for <X> years, and that there have been many various papers and theses
 produced regarding these issues, this appears the logical path for improvements
 to take.
 Should any future developers decide to follow through on the same path,
 this paper has logically set out its concepts and ideas, and will provide
 a solid basis for such research.
\end_layout

\begin_layout Standard
As the topic of data mining, and more specifically, clustering, becomes
 more of a part of every-day business and organisational operation, the
 necessity for faster and equally accurate algorithms rises.
 This paper has shown this, and several other applications do exist, and
 it can only be assumed that 
\end_layout

\begin_layout Standard
asf 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "clustersdk"
options "latex8"

\end_inset


\end_layout

\end_body
\end_document
