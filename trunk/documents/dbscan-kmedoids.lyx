#LyX 1.6.0rc3 created this file. For more info see http://www.lyx.org/
\lyxformat 340
\begin_document
\begin_header
\textclass latex8
\begin_preamble

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%


\usepackage{latex8}

\usepackage{times}

\usepackage{epsf}

\usepackage{epsfig}

\usepackage[config, font={sf,bf}]{caption}

\usepackage[config, font={small, sf}]{subfig}

\usepackage{latexsym}
   % additional packages that may be read in
    % to augment generic LaTeX; needed for \mathbb

\newcommand{\VD}{{Voronoi diagram}}
\newcommand{\VDs}{{Voronoi diagrams}}
%\documentstyle[times,art10,twocolumn,latex8]{article}

%-------------------------------------------------------------------------
% take the % away on next line to produce the final camera-ready version


%-------------------------------------------------------------------------
\end_preamble
\options times
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle empty
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Clusterers: a Comparison of Partitioning and Density-Based Algorithms and
 a Discussion of Optimisations
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
author{David Breitkreutz
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{
\backslash
& Information Technology} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia
\backslash

\backslash
 David.Breitkreutz@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
and
\end_layout

\begin_layout Plain Layout

Kate Casey
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit {
\backslash
& Information Technology} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Kate.Casey@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Though data mining is a relatively recent innovation, the improvements it
 offers over traditional data analysis have seen the field expand rapidly.
 Given the critical requirement for the efficient and accurate delivery
 of useful information in today's data-rich climate, significant research
 in the topic continues.
\end_layout

\begin_layout Abstract
Clustering is one of the fundamental techniques adopted by data mining tools
 across a range of applications.
 It provides several algorithms that can assess large data sets based on
 specific parameters and group related data points.
\end_layout

\begin_layout Abstract
This paper compares two widely used clustering algorithms, K-Medoids and
 Density-Based Spatial Clustering of Applications with Noise (DBSCAN), against
 other well-known techniques.
 The initial testing conducted on each technique utilises the standard implement
ation of each algorithm.
 Further experimental work proposes and tests potential improvements to
 these methods, and presents the UltraK-Medoids and UltraDBScan algorithms.
 Various key applications of clustering methods are detailed, and several
 areas of future work have been suggested.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Introduction"

\end_inset

Introduction 
\end_layout

\begin_layout Standard
Traditional statistical analysis relies on confirmatory methods; applying
 pre-defined mathematical models to data sets in order to identify trends.
 This technique, while effective and extensively used, has limitations when
 presented with very large data sets.
 Data mining, however, is based on an exploratory framework and analyses
 data on an adhoc basis.
 This provides additional flexibility to the suite of algorithms, and allows
 efficient processing of large reserves of data.
\end_layout

\begin_layout Standard
As a subsidiary of data mining, clustering is promoted as an extremely powerful
 means of grouping related data points.
 The technique can efficiently reveal highly relevant trends in a source
 data set, and this capability extends to the large data repositories used
 by scientists, researchers, and businesses.
 
\end_layout

\begin_layout Standard
As a result, the field has developed into one of the foremost research areas
 in modern computing.
 There exist numerous algorithms for performing such analysis, and each
 may be more suitable to certain circumstances, depending entirely upon
 domain-specific parameters.
 As previously discussed, these techniques can analyse both large-scale
 databases and data warehouses.
 
\end_layout

\begin_layout Standard
The efficiency and accuracy of results from these data mining tasks relies
 directly upon the choice of a suitable algorithm.
 Thus, given the many different types of data sets that exist, there is
 a strong requirement for further research into the improvements of these
 techniques.
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Data mining in general has a multitude of applications across a wide variety
 of fields.
 Pattern recognition for image analysis, medical diagnostics, sales forecasting
 and weather prediction are a recognised as a few of the more traditional
 usages.
 However, due to extensive development in the field and the recent explosion
 in data recording, the capabilities extend far beyond these basic functions.
 On-board computer analysis in vehicles, product quality analysis, targeted
 advertising campaigns, spam email filtration, fraud detection, and online
 crime analysis are but a few of the fields into which data mining now extends
 
\begin_inset CommandInset citation
LatexCommand cite
key "john1999bsd"

\end_inset

.
 Clustering applies to all of the aforementioned applications as a subset
 of data mining.
 
\end_layout

\begin_layout Standard
A prominent example of this functionality is the application of emergency
 situation data analysis; for example, data recorded by authorities during
 forest fires.
 This paper will investigate that specific scenario in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Details"

\end_inset

, with the test sets utilised containing fire data from Portugal
\emph on
.
\end_layout

\begin_layout Subsection
Other methods
\end_layout

\begin_layout Standard
There are a wide variety of clustering methodologies that exist within this
 field.
 These include, but are not limited to, grid-based categorisation, density-based
 grouping, hierarchical segmentation, and constraint-based analysis.
 As these techniques have been already widely researched, there are many
 existing works that compare and contrast each.
 The interested reader is directed to 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 for an expansive summary of these topics.
\end_layout

\begin_layout Standard
The research outlined in this paper is concerned with the application of
 two of the most widely used methods; 
\emph on
partitioning-based 
\emph default
and 
\emph on
density-based
\emph default
 clustering.
\end_layout

\begin_layout Standard
Partitioning algorithms are effective for mining data sets when computation
 of a clustering tree, or 
\emph on
dendrogram
\emph default
, representation is infeasible 
\begin_inset CommandInset citation
LatexCommand cite
key "jain1999dcr"

\end_inset

.
 In particular, K-Medoids is highly efficient for smaller data sets with
 spherical-type clusters.
 However, due to the inherent swapping of medoids to optimise the clustered
 solution, the algorithm suffers greatly as large data sets are introduced.
 
\end_layout

\begin_layout Standard
Density-based algorithms perform optimally when operating upon spatially-indexed
 data.
 The methods provide benefits when analysing data sets that contain high
 levels of noise or when clusters are arbitrarily shaped.
 Specifically, DBSCAN is able to grow clusters within a specified neighbourhood
 whilst the minimum points threshold is not satisfied, thus efficiently
 dividing real data and noise in a variety of shapes.
\end_layout

\begin_layout Subsection
Purpose of paper
\end_layout

\begin_layout Standard
This paper focuses upon the DBSCAN and K-Medoids algorithms from the density-bas
ed and partitioning families respectively.
 It outlines the design, advantages and disadvantages of each method, and
 studies similar methodologies.
 The work contained goes on to propose both basic implementations of the
 techniques, as well as initiating new research into improving the efficiency
 of the aforementioned methods.
 A great deal of literature already exists regarding these techniques, and
 a review of these documents can be found in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Data-Set"

\end_inset

Experimental data set
\end_layout

\begin_layout Standard
The data set chosen for the comparison of and amendments to the K-Medoids
 and DBSCAN techniques is 
\emph on
forest fires
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "cortez:dma"

\end_inset

.
 The set contains environmental data from fire instances in northeastern
 Portugal; representing instances within space, and including attributes
 such as date, temperature, humidity, wind conditions, rain information
 and burned area.
 
\end_layout

\begin_layout Standard
Analysing this data set is necessary as forest fires cause millions of dollars
 worth of damage to property and claim many lives each year.
 If clustering techniques can be used to more accurately determine the patterns
 of such fires given prevailing environmental conditions, scientists and
 researchers will be able to achieve further understanding of the phenomenon.
 This knowledge will reap enormous benefits; predictive techniques can be
 updated, analysis centres local to fires can more accurately forecast fire
 behaviour, government agencies can be continuously informed of these conclusion
s, and residents and business owners can be warned earlier.
 This improved system will result in reduced property damage, lower restoration
 costs and, ultimately, fewer lives lost.
\end_layout

\begin_layout Subsection
Results overview
\end_layout

\begin_layout Standard
The results explained in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

 form the basis of the experimental portion of this paper.
 Based on the forest fires data set, these figures reflect the implementation
 results of the DBSCAN and K-Medoids algorithms.
 
\end_layout

\begin_layout Standard
In addition, testing of the modified algorithms this paper suggests has
 shown a level of computational improvement for both methods when clustering
 the aforementioned data set.
 These findings are detailed and discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Related-Work"

\end_inset

Related Work
\end_layout

\begin_layout Subsection
Data analysis
\end_layout

\begin_layout Standard
A formal definition by Moore 
\begin_inset CommandInset citation
LatexCommand cite
key "moore1998pas"

\end_inset

 states that data analysis is 
\emph on

\begin_inset Quotes eld
\end_inset

the examination of data for interesting patterns and striking deviations
 from those patterns
\begin_inset Quotes erd
\end_inset


\emph default
.
 Such processing can lead to conclusions which assist a multitude of people
 in varying tasks; from medical diagnostics to population growth predictions
 to forest fire forecasting.
 The potential developments in these research areas due to quality data
 analysis are not only beneficial in terms of further knowledge and understandin
g, but will inevitably benefit ordinary people in ways beyond the scope
 of this paper.
 Data mining is a subset of the broader data analysis field, and as such
 can provide the advantages previously discussed when large data sets require
 processing.
\end_layout

\begin_layout Subsection
Clustering
\end_layout

\begin_layout Standard
Aggarwal et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "aggarwal1999fap"

\end_inset

 define the process of clustering as:
\emph on
 
\begin_inset Quotes eld
\end_inset

Given a set of points in multidimensional space, find a partition of the
 points into clusters so that the points within each cluster are close to
 one another
\begin_inset Quotes erd
\end_inset


\emph default
.
 Proximity is measured using a variety of algorithm-specific metrics, such
 that the closer two arbitrary points are to one another, the more strongly
 they are considered to be related.
 This process results in defined groupings of similar points, where strong
 inter-cluster and weak intra-cluster relationships exist among and between
 points, an example of which is detailed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-clustered"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename clustering.png
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-clustered"

\end_inset

Example of clustered points
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clustering can be categorised in machine learning terms as a form of 
\emph on
unsupervised learning
\emph default
; that is, clusters are representative of hidden patterns in the source
 data set
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

.
 Raw information is analysed and relationships are discovered by the algorithm
 without external direction or interference; learning through observation
 rather than by the study of examples
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 In addition, this objectivity translates to an effective means of data
 analysis without the opportunity for subjective human conclusions to be
 drawn from data.
\end_layout

\begin_layout Subsection
Partitioning clustering algorithms
\end_layout

\begin_layout Standard
Partitioning methods define clusters by grouping data points into 
\emph on
k
\emph default
 partitions, defined by the user at the time the process is executed.
 A point is determined to be 
\emph on
similar
\emph default
 to other points within its partition, and 
\emph on
dissimilar
\emph default
 to points that lie outside the boundary of that partition 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 Comparison is based on the characteristics of the data set provided.
 Thus, the algorithms rely on the conversion of semantic data attributes
 (width, height, shape, colour, cost or others) into points that determine
 physical location on a set of mathematical axes.
 This provides an objective and computationally acceptable framework for
 analysis.
 In the simplest case only two attributes exist, and thus the conversion
 renders a point on a standard Cartesian plane.
 This process is greatly complicated when, as often occurs in highly detailed
 source sets, hundreds of attributes are present.
 The rendering plane takes on high dimensionality, and the complexity of
 analysis becomes very computationally expensive.
 
\end_layout

\begin_layout Standard
Partitioning offers several features, including efficiency in processing
 spherical clusters in small to medium data sets, and scalability across
 smaller applications.
 However, the algorithms suffer from extreme complexity and computational
 cost at higher dimensions in large sets, require the user to specify the
 number of clusters to be determined (
\emph on
k
\emph default
) and experience problems defining arbitrarily shaped clusters.
\end_layout

\begin_layout Standard
Previous work has outlined several clustering methods within the partitioning
 family; 
\emph on
K-means
\emph default
 converts data to numerical points on a set of axes and calculates the numerical
 mean of these points to dictate cluster centroids 
\begin_inset CommandInset citation
LatexCommand cite
key "hartigan1979kmc"

\end_inset

, 
\emph on
K-modes 
\emph default
is similar to K-means though it calculates the mode of point groupings instead
 of the mean to define cluster centroids , and 
\emph on
K-medoids
\emph default
 converts data points to objects on a set of axes and arbitrarily chooses
 different objects to act as cluster centroids until the clusters are optimised
 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1987cmm"

\end_inset

.
 As K-Medoids provides dramatic improvements in noise immunity over its
 other partitioning counterparts, a great deal of further research has been
 pursued including the forthcoming work in this paper.
 
\emph on
Partitioning Around Medoids 
\emph default
(PAM) randomly chooses medoids, then randomly calculates the result of swapping
 these with other random objects in an attempt to improve the clustering
 outcome 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1990pam"

\end_inset

.
 This approach works adequately for small source sets, but does not scale
 well as complexity at each iteration is quadratic.
 
\emph on
Clustering LARge Applications
\emph default
 (CLARA)
\emph on
 
\emph default
takes a single random selection of points in the set as representative data
 and then applies PAM to this smaller subset 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1990fgd"

\end_inset

.
 Reducing the size of the set for analysis improves scalability as complexity
 at each iteration is linear, but the method suffers depending upon the
 quality of sample data chosen.

\emph on
 Clustering Large Applications based on RANdomised Search
\emph default
 
\emph on
(
\emph default
CLARANS)
\emph on
 
\emph default
takes a random dynamic selection of data at each step of process â€“ thus
 the same sample set is not used throughout the clustering process 
\begin_inset CommandInset citation
LatexCommand cite
key "ng1994eae"

\end_inset

.
 As a result, better randomisation of source data is achieved, but the method
 still suffers depending on the random selection and can be very slow as
 the algorithm's time complexity is approximately quadratic.
 
\emph on
UltraK-Medoids
\emph default
, the algorithm proposed in this paper, makes improvements to the basic
 K-Medoids method by performing reassignment of medoids and not assessing
 the effect on the data space through recalculation.
 This provides performance advantages, reducing time complexity to linear,
 but suffers when data sets are vastly dispersed.
\end_layout

\begin_layout Subsection
Density-based clustering algorithms
\end_layout

\begin_layout Standard
The density-based group of clustering algorithms represent a data set in
 the same manner as partitioning methods; converting an instance to a point
 using the data attributes of the source set.
 The plane contains clusters with high internal density and low external
 density in a similar manner to its partitioning ancestor 
\begin_inset CommandInset citation
LatexCommand cite
key "Kolatch01clusteringalgorithms"

\end_inset

.
 The process of adding points to a cluster is iterative, unlike partitioning
 methods.
 Nearest neighbours of each point can thus be investigated, arbitrary shapes
 formed, and existing clusters merged as the algorithm moves through all
 points.
 As a result, analysis can easily isolate noise instances from relevant
 data, whilst being able to cluster data object sets that include hollow
 formations.
 
\end_layout

\begin_layout Standard
Density-based algorithms provide advantages over other methods through their
 noise handling capabilities and ability to determine clusters with arbitrary
 shapes (eg.
 a hollow circle, torus or other non-convex formation).
 As with partitioning techniques, computational cost is a disadvantage when
 the technique is used with large amounts of source data and sets containing
 excessive noise.
 
\end_layout

\begin_layout Standard
Literature exists on several techniques that utilise the density-based clusterin
g concept.
 
\emph on
DENsity based CLUstEring 
\emph default
(DENCLUE)
\emph on
 
\emph default
uses multiple clustering paradigms (including density-based, partitioning
 and hierarchical), is based on a set of density distribution functions,
 and uses influence functions between points to model the data space 
\begin_inset CommandInset citation
LatexCommand cite
key "hinneburg1998eac"

\end_inset

.
 The method has a strong mathematical foundation which provides advantages
 in terms of set representation, but density and noise parameter selection
 can adversely affect the average linear time complexity.
 
\emph on
Density-Based Spatial Clustering of Applications with Noise
\emph default
 (DBSCAN) locates and grows regions with sufficient density as specified
 by input parameters 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

.
 The algorithm does not require that all points are allocated to clusters
 which provides tolerance to noise and supports clustering of arbitrary
 shapes, but can suffer if input parameters are selected incorrectly.
 Time complexity is typically 
\begin_inset Formula $O(n\log n)$
\end_inset


\emph on
 
\emph default
but can be as poor as quadratic when a spatial index is not used.
 DBSCAN is widely used due to its ability to handle noise and define clusters
 with non-standard shapes, and thus many extensions to its algorithm are
 defined in the literature.
 
\emph on
Ordering Points To Identify the Clustering Structure
\emph default
 (OPTICS) is similar to DBSCAN but produces augmented cluster ordering instead
 of defining actual clusters
\begin_inset CommandInset citation
LatexCommand cite
key "ankerst1999oop"

\end_inset

.
 This approach is advantageous as the ordering can be used to both derive
 key cluster characteristics and analyse the structure of the cluster space,
 and time complexity is the same as DBSCAN; averaging 
\begin_inset Formula $O(n\log n)$
\end_inset

.
 
\emph on
Generalized Density-Based Spatial Clustering of Applications with Noise
\emph default
 (GDBSCAN)
\emph on
 
\emph default
can cluster both polygons and points, and does not rely on any set definition
 of a neighbourhood; the method can use non-spatial attributes rather than
 number of objects to define neighbourhood density 
\begin_inset CommandInset citation
LatexCommand cite
key "sander1998dbc"

\end_inset

.
 The technique does not limit clusters to a maximum radius and thus achieves
 independence from a neighbourhood size input variable.
 Time complexity is similar to DBSCAN, though 
\begin_inset Formula $O(n\log n)$
\end_inset

 is achieved only using RTrees 
\begin_inset CommandInset citation
LatexCommand cite
key "berry2006lnd"

\end_inset

.
 
\emph on
UltraDBScan
\emph default
, the algorithm proposed in this paper, introduces the concept of a dynamic
 cluster radius specification.
 Changing this value based on the number of points in the cluster as the
 algorithm moves through all its points allows dynamic removal of noise.
 This is highly advantageous in sets with closely-bound clusters, but may
 result in the declaration of valid points as noise in more dispersed sets.
 
\end_layout

\begin_layout Subsection
Alternative clustering methods
\end_layout

\begin_layout Standard
Two of the most popular clustering methods in use other than density-based
 and partitioning
\emph on
 
\emph default
are 
\emph on
hierarchical 
\emph default
and 
\emph on
grid-based
\emph default
.
\end_layout

\begin_layout Standard
Hierarchical algorithms can define clusters in two ways; either top-down
 (
\emph on
divisive
\emph default
) or bottom-up (
\emph on
agglomerate
\emph default
).
 Divisive techniques define a broader cluster first which encompasses all
 points, then split this into more specialised child clusters.
 These child clusters in turn spawn their own child clusters and the process
 continues until all points are accounted for.
 The agglomerate technique is the reverse, defining the smallest and lowest-leve
l clusters first and working upward until the entire set is contained in
 one cluster 
\begin_inset CommandInset citation
LatexCommand cite
key "grabmeier2002tca"

\end_inset

.
 The methods are advantageous in terms of processing time efficiency, but
 accuracy suffers as they do not allow for incorrect clustering decisions
 to be reversed (back-traversal).
 Chameleon 
\begin_inset CommandInset citation
LatexCommand cite
key "karypis1999chc"

\end_inset

 and BIRCH 
\begin_inset CommandInset citation
LatexCommand cite
key "zhang1997bnd"

\end_inset

 are examples of hierarchical methods; Chameleon concerns analysis of inter-obje
ct relationships at each hierarchical level and BIRCH utilises iterative
 relocation.
\end_layout

\begin_layout Standard
Grid-based techniques divide the data space into a finite number of cells
 (hence 
\emph on
grid
\emph default
) and clustering methods are applied to these segments.
 Due to this division of study space, such algorithms are dependent in terms
 of processing only on the quantity of cells at any dimension.
 Thus the technique has very high computational processing efficiency 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 The WaveCluster 
\begin_inset CommandInset citation
LatexCommand cite
key "sheikholeslami1998wmr"

\end_inset

 and STING 
\begin_inset CommandInset citation
LatexCommand cite
key "wang1997ssi"

\end_inset

 algorithms both employ grid-based clustering techniques; WaveCluster utilises
 wavelet transformations on source data and STING exhibits typical grid-based
 analysis behaviour.
\end_layout

\begin_layout Subsection
Alternative data mining techniques
\end_layout

\begin_layout Standard
Clustering differs from other classical data mining methodologies in a variety
 of ways and extensive literature already exists explaining this.
 For the purpose of conciseness, this paper will compare and contrast clustering
 only with the techniques of 
\emph on
classification
\emph default
 and 
\emph on
prediction
\emph default
.
\end_layout

\begin_layout Standard
Classification is the process of constructing a learning model, or 
\emph on
classifier
\emph default
, that assigns points in a data set to predefined classes
\begin_inset CommandInset citation
LatexCommand cite
key "fayyad1996dma"

\end_inset

.
 Whilst this definition initially appears similar to that of clustering
 in that points are grouped based on attributes, any further similarities
 are thus exhausted.
 The technique utilises 
\emph on
supervised learning 
\emph default
as opposed to the unsupervised learning environment of clustering.
 The model is compiled by studying provided data (known as a training set)
 containing class labels; hence the technique is 
\begin_inset Quotes eld
\end_inset

supervised
\begin_inset Quotes erd
\end_inset

 due to example data exposure.
 Once the model is complete it can be applied to real, labeled data and
 assign each new tuple to its respective class
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 Classification is used in the categorisation of discrete or unordered data
 sets; for example, determining the level of risk (high/medium/low) associated
 with a certain financial investment given pre-existing market data.
 
\end_layout

\begin_layout Standard
Prediction is similar to classification, in that the process of learning
 from existing data is supervised, and a model is constructed.
 However, prediction techniques are used when assessing continuous data
 sets 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

; for example, determining the price of a stock market share at some point
 in the future given previous performance trends.
 Unlike prediction, clustering is not exclusively used for forecasting future
 values, rather grouping like points for immediate assessment.
\end_layout

\begin_layout Standard
Whether to utilise clustering in analysis is a decision based on the purpose
 of the data mining task.
 If future predictions are required, an alternative technique should be
 employed, but if like points in the set are to be grouped, clustering is
 an appropriate choice.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Comparison"

\end_inset

Comparison
\end_layout

\begin_layout Standard
This paper aims to draw a detailed comparison between two general clustering
 paradigms - density-based and partitioning - and in order to do so, examines
 the differences between several other techniques.
 By examining the differences in how each method operates, their varying
 capabilities and the circumstances in which they are most applicable will
 be revealed.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:DBSCAN"

\end_inset

DBSCAN
\end_layout

\begin_layout Standard
The DBSCAN clustering technique, first proposed in 1996 by Ester et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

, requires the user to specify two parameters: 
\emph on
minPoints
\emph default
, defined as the minimum number of points required to exist in a neighbourhood
 to be declared a cluster, and 
\begin_inset Formula $\varepsilon$
\end_inset

, defined as the radius of the neighbourhood of a point based on a distance
 metric.
 The minimum number of points is used to determine if an arbitrary collection
 of points should be considered a cluster.
 The 
\begin_inset Formula $\varepsilon$
\end_inset

 value controls the radius around an object, whether it be in Euclidean,
 Manhattan or Minkowski distance, in which the algorithm should consider
 other points as neighbours.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:DBSCAN"

\end_inset

DBSCAN
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
For each unvisited point 
\begin_inset Formula $P$
\end_inset

 within the data set 
\begin_inset Formula $D$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Get the neighbours of the 
\begin_inset Formula $P$
\end_inset

, according to the given epsilon distance
\end_layout

\begin_layout Enumerate
If the number of neighbours is equal or greater than the user-specified
 cluster threshold
\end_layout

\begin_deeper
\begin_layout Enumerate
Increment cluster identifier counter
\end_layout

\begin_layout Enumerate
Add the given point to the current cluster
\end_layout

\begin_layout Enumerate
Recursively process all neighbours 
\end_layout

\end_deeper
\begin_layout Enumerate
Else if the point has fewer neighbours than the threshold value,
\end_layout

\begin_deeper
\begin_layout Enumerate
Mark 
\begin_inset Formula $P$
\end_inset

 as noise
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Standard
The process, as detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DBSCAN"

\end_inset

, begins by considering an arbitrary data object that has not already been
 assigned to a cluster.
 The neighbours of this data point 
\begin_inset Formula $P$
\end_inset

 are then located, according to the user-specified 
\begin_inset Formula $\varepsilon$
\end_inset

 value.
 If the number of neighbours found is greater than the specified 
\emph on
minPoints
\emph default
 value, then the current data object should be added to the present cluster,
 and all neighbours should be processed recursively.
 Otherwise, if the number of nearby points is less than 
\emph on
minPoints
\emph default
, then the data object is considered noise.
 This process then continues for all of the neighbours, processing them
 accordingly.
 At such time that a given cluster has been fully explored, the process
 begins again, until all points have been visited.
 At its conclusion, all defined clusters are finalised; no further points
 can be added to any cluster and any unclustered points are declared as
 
\emph on
noise
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscan.png
	lyxscale 50
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-stage-of"

\end_inset

Initial stage of DBSCAN technique
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
An initial processing stage for the DBSCAN procedure, performed on a simplified
 data set, is shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-stage-of"

\end_inset

.
 The dashed line represents a two-dimensional 
\begin_inset Formula $\varepsilon$
\end_inset

 distance radius and the minimum points threshold is 1.
\end_layout

\begin_layout Subsection
OPTICS
\end_layout

\begin_layout Standard
The Ordering Points to Identify the Clustering Structure (OPTICS) algorithm
 is procedurally identical to that of the previously mentioned DBSCAN 
\begin_inset CommandInset citation
LatexCommand cite
key "ankerst1999oop"

\end_inset

.
 Thus its algorithm is similar to that shown in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DBSCAN"

\end_inset

, and its time complexity is the same.
 
\end_layout

\begin_layout Standard
The OPTICS technique builds upon DBSCAN by introducing values that are stored
 with each data object; an attempt to overcome the necessity to supply different
 input parameters.
 Specifically, these are referred to as the 
\emph on
core-distance
\emph default
, the smallest epsilon value that makes a data object a core object, and
 the 
\emph on
reachability-distance, 
\emph default
which is a measure of distance between a given object and another.
 The 
\emph on
reachability-distance 
\emph default
is calculated as the greater of either the 
\emph on
core-distance
\emph default
 of the data object or the Euclidean distance between the data object and
 another point
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 
\end_layout

\begin_layout Standard
These newly introduced distances are used to order the objects within the
 data set.
 Clusters are defined based upon the reachability information and core distances
 associated with each object; potentially revealing more relevant information
 about the attributes of each cluster.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:K-Medoids"

\end_inset

K-Medoids
\end_layout

\begin_layout Standard
The K-Medoids clustering technique is concerned with the concept of partitioning
 data objects into similar groups.
 The technique was first developed in 1987 by Kaufman and Rousseeuw 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1987cmm"

\end_inset

 and seeks to reduce the impact of noisy data experienced in simpler partitionin
g algorithms, such as K-Means.
 Instead of converting data attributes to simple points, it retains all
 the information from each record and represents the points as objects on
 a detailed axes.
 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:K-Medoids"

\end_inset

K-Medoids
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily select 
\begin_inset Formula $k$
\end_inset

 objects as initial medoid points from given data set
\end_layout

\begin_layout Enumerate
Whilst swaps are occurring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data object in the data set with its closest medoid
\end_layout

\begin_layout Enumerate
Randomly select a non-medoid object, 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:Compute-the-total"

\end_inset

Compute the total cost of swapping initial medoid object with 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
If the total cost of swapping is less than the total cost of the current
 system then
\end_layout

\begin_deeper
\begin_layout Enumerate
Swap the initial medoid with 
\begin_inset Formula $O$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Continue until there is no change
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Standard
This method, as detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Medoids"

\end_inset

, segments the given database of objects into a user-specified number of
 clusters (
\begin_inset Formula $k$
\end_inset

).
 Each cluster exists such that all contained data objects are most closely
 associated with the medoid of the cluster.
 The process begins with the initial selection of 
\emph on
k
\emph default
 arbitrary points as representative medoids, and then computes cluster assignmen
ts for each of the data objects.
 At this stage, each point will be associated with a medoid from the 
\emph on
k
\emph default
 medoids available, as demonstrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kmedoids-clustering-process"

\end_inset

.
 In this example, the two marked data objects are considered to be medoids
 of their respective clusters.
 During this initial stage of the algorithm, the surrounding objects are
 assigned to their closest respective medoid.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoids.png
	lyxscale 50
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kmedoids-clustering-process"

\end_inset

K-Medoids clustering process
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The algorithm then continues by selecting a random non-medoid data object,
 O
\begin_inset Formula $_{r}$
\end_inset

, and recalculating the potential benefit were it swapped with its current
 medoid.
 If there is such a benefit, the system should make the change and continue.
 This process continues until the system is in such a state that every data
 object has been assigned to a cluster, and further swapping would provide
 no further optimisation to the clustering outcome 
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

.
\end_layout

\begin_layout Standard
The calculation of prospective benefit after swapping is based on a cost
 function; the total distance between all points and their respective medoids.
 In a similar manner to DBSCAN, the term 
\emph on
distance
\emph default
 used in this case may refer, depending upon the implementation, to a Euclidean,
 Manhattan or Minkowski metric.
\end_layout

\begin_layout Subsection
K-Means
\end_layout

\begin_layout Standard
The K-Means technique is similar to that of K-Medoids in that both methods
 use the same designations of input variables, and both attempt to cluster
 data objects in a similar manner.
 Whilst K-Medoids is concerned with using data objects themselves as cluster
 mid-points, K-Means, utilises the centroid, or 
\emph on
mean
\emph default
, of a cluster for its computations.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:K-Means"

\end_inset

K-Means
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily or heuristically select 
\begin_inset Formula $k$
\end_inset

 objects as initial centroids for the given data set
\end_layout

\begin_layout Enumerate
Whilst centroid changes are occurring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data point 
\begin_inset Formula $P$
\end_inset

 in the data set to its closest centroid
\end_layout

\begin_layout Enumerate
For each centroid 
\begin_inset Formula $C$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Recompute location according to data points associated with it
\end_layout

\end_deeper
\begin_layout Enumerate
Continue until stopping criteria met
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Standard
The method, as described in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Means"

\end_inset

, begins as the K-Medoids method does - by using some process to select
 
\emph on
k
\emph default
-many objects as initial mean objects from the data set.
 From this, each data object is iteratively assigned to its respective cluster,
 according to its nearest centroid point.
 Once this process has completed, the centroid for each cluster is recomputed
 accordingly.
 This process will continue until the data set has reached a state of convergenc
e; some stopping condition has been reached, and the clustering results
 have thereby been sufficiently optimised 
\begin_inset CommandInset citation
LatexCommand cite
key "Hartigan1979"

\end_inset

.
\end_layout

\begin_layout Subsection
Capability comparison
\end_layout

\begin_layout Standard
Directing responsibility for the choice of correct input parameters toward
 the user has clear disadvantages.
 Given different circumstances in terms of data shapes and data set size,
 markedly different results can be seen using the same algorithm.
 K-Means, K-Medoids and UltraK-Medoids require the user to nominate a desired
 number of clusters, and an incorrect choice can seriously compromise results.
 If too high a number is chosen, related points may be clustered apart,
 whilst too low a choice may result in noise points being allocated to a
 legitimate cluster.
 DBSCAN and OPTICS support location of an arbitrary number of clusters (no
 
\emph on
k
\emph default
-cluster limitation), but they require both a cluster radius and minimum-points-
per-cluster specification, and thus suffer similar problems in terms of
 cluster accuracy to the K-algorithms.
 Incorrect choice of these variables can lead to effects on the final clusters
 similar to those experienced when using partitioning techniques.
 UltraDBScan, however, alleviates this issue and improves noise removal
 through allowing dynamic change of 
\begin_inset Formula $\varepsilon$
\end_inset

 by the algorithm during processing.
 The minimum points variable is still required, though the effect of a pre-defin
ed 
\begin_inset Formula $\varepsilon$
\end_inset

 is reduced.
 This is highly effective when analysing tightly-grouped sets, but may lead
 to relevant relationships being discarded in less compact sets.
\end_layout

\begin_layout Standard
K-algorithms require that all points in the set are clustered, whereas DBSCAN
 and its descendants do not.
 This disadvantages the K-series as even noise points are required to be
 assigned to clusters, which compromises the quality of legitimate clusters.
 DBSCAN-type algorithms are not susceptible to this issue due to their design,
 and will correctly discard such points as noise.
\end_layout

\begin_layout Standard
The DBSCAN algorithm, like other density-based techniques, is most suited
 to data sets where the number, shape, or size of clusters present are not
 known, and the presence of noise would otherwise severely affect results.
 These are two benefits which are not available from the partitioning techniques
 - K-Medoids or K-Means - under review.
 Both of these algorithms are sensitive to noise, though K-medoids has slightly
 improved outlier tolerance than K-Means.
 
\end_layout

\begin_layout Standard
K-Means will scale more effectively than K-medoids due to its reduced processing
 requirements, however density-based algorithms are far more applicable
 to larger data sets due to their inherently robust nature.
 The K-algorithms work very well for smaller data sets (in terms of both
 dimensionality and quantity of records), but lose efficiency as the size
 of the source set increases.
 In terms of efficiency, which is directly related to scalability, K-algorithms
 perform better than density-based for small sets, but are surpassed in
 performance by DBSCAN and OPTICS when larger sets are introduced.
 This conclusion is supported by the time-complexity metric for each technique;
 K-algorithms operate in quadratic time, whereas density-based have 
\begin_inset Formula $O(n\log n)$
\end_inset

 average times, and quadratic only in worst-case scenarios.
 
\end_layout

\begin_layout Standard
Given the aforementioned observations and conclusions, traditional K-algorithms
 are recommended for low-dimensional data sets with few records, low noise
 and spherical data groupings.
 The original density-based algorithms are well-equipped to handle more
 expansive sets with higher dimensionality, larger quantities of tuples,
 widespread noise and arbitrarily-shaped cluster formations.
 Of the adapted algorithms proposed in this paper, UltraK-Medoids is recommended
 for the same style of data set as its ancestors, while UltraDBScan is recommend
ed for lower-noise sets of the type suited to its ancestors.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Customisations"

\end_inset

Customisations
\end_layout

\begin_layout Standard
As previously mentioned, the subject area under consideration has already
 been extensively investigated by other authors over time.
 Many previous works have discussed ways in which clustering algorithms
 can be optimised and made otherwise more resilient.
 This paper attempts to add to the already voluminous amount of work in
 the field of clustering by introducing two new techniques.
 These new methods are derived from the existing DBSCAN and K-Medoids algorithms
, which were chosen for analysis as they both operate successfully, return
 useful results, and are well established and understood throughout the
 existing literature.
\end_layout

\begin_layout Standard
It should be noted that these customisations aim only to modify the aforemention
ed techniques, rather than to construct extended methodologies.
 This choice is justified by the fact that any significantly complex modificatio
n to an existing clustering technique would affect its speed and performance,
 whilst leaving core functionality the same.
 As this paper focuses upon performance as a key aspect of clustering algorithms
, research beyond the scope of the work contained herein would be required
 to investigate further performance enhancements.
 Thus, whilst this paper does not suggest entirely new clustering techniques,
 the modifications proposed do appear to have significant respective uses.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Customisations-DBSCAN"

\end_inset

DBSCAN
\end_layout

\begin_layout Standard
Some authors 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 highlight that an issue which affects DBSCAN, and many other clustering
 techniques, is the necessity to correctly select input parameters.
 In the case of DBSCAN, these values are the minimum number of points needed
 to form a cluster, 
\emph on
minPoints,
\emph default
 and the distance value, 
\begin_inset Formula $\varepsilon$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Customisations-DBSCAN"

\end_inset

.
 As expected, if a set of input variables are incorrectly selected - whereby
 the margin between correct and incorrect may vary greatly, depending on
 the data set - the results produced may be completely unexpected, and in
 some cases rendered useless.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanultra.png
	lyxscale 50
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Customised-DBSCAN-procedure"

\end_inset

Customised DBSCAN procedure
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This paper suggests that the DBSCAN algorithm be modified to offer dynamic
 change in 
\begin_inset Formula $\varepsilon$
\end_inset

 distance, rather than using a static, potentially unsuitable value for
 all data objects within a collection.
 To achieve this change, the distance value is adjusted according to the
 number of points present within the current cluster.
 This modification aims to decrease the distance checked around each point
 within a cluster, in order to reduce the risk of data instances being included
 in a cluster to which they do not belong.
 Consider a dense cluster of objects, such as that in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Customised-DBSCAN-procedure"

\end_inset

, and that this cluster also has a number of outlier objects surrounding
 it.
 Using a constant distance value in this situation would be ineffective
 and result in the objects marked as noise being assigned to the cluster
 incorrectly.
 The consideration in these situations is that a very dense cluster will
 see its members similarly spaced.
 For any given 
\begin_inset Formula $\varepsilon$
\end_inset

, if many data objects are found using such a value, 
\begin_inset Formula $\varepsilon$
\end_inset

 can be decreased to reduce the effect of noise, and lessen the potential
 for the 
\emph on
bridging effect
\emph default
, whereby two clusters are inadvertently joined.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Customisations-K-Medoids"

\end_inset

K-Medoids
\end_layout

\begin_layout Standard
As previously mentioned in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:K-Medoids"

\end_inset

, one of the biggest issues that affects the K-Medoids algorithm is the
 computational time required to determine the suitability of a given random
 object becoming a new medoid 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 This overhead is largely unnecessary, as a system will frequently be in
 such a state that the reassignment of a given medoid will not affect a
 large amount of the data objects present.
 This would be especially relevant for a large data set that consists of
 many clusters; a single medoid change would only affect the small number
 of objects and clusters nearby.
 
\end_layout

\begin_layout Standard
Within the original algorithm, detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Medoids"

\end_inset

, the step described at 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:Compute-the-total"

\end_inset

 has been determined as the most superfluous.
 At present, calculating the cost of swapping one medoid for another is
 significant, with the time complexity of this one sub-operation being 
\begin_inset Formula $O(n)$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of objects present within the given database.
 When one considers this operation in context - that the entire set of data
 objects need to be iterated over each time a swap, or potential swap, occurs
 - the overhead is evident.
\end_layout

\begin_layout Standard
A suitable customisation to the K-Medoids procedure is an attempt to limit
 the number of times that data objects need to be processed, per iteration
 of swaps of medoids.
 This suggested concept requires the clustering associations, and thereby
 the distances from a given data object to its related medoid, be recalculated
 at most only once when a medoid object is changed.
 At present, the algorithm recomputes the cluster and medoid assignments
 for all points in two instances: initially or after a swap has occurred,
 and also in order to determine the total cost of swapping a medoid within
 the system.
 In many situations, this would be grossly inefficient as most points should
 and will remain associated with the same medoid.
 The only data instances that would change are those which have close to
 dual-medoid membership, given the location of the new, potential medoid.
 With the suggested customisations, the algorithm only recalculates cluster
 membership after a change has been made, and rather than recomputing all
 memberships at the second instance, it instead swaps a given medoid out
 temporarily with the other randomly selected medoid.
\end_layout

\begin_layout Standard
Because the second iteration through the entire data set is made redundant,
 the act of swapping medoids can be performed at the same time that clustering
 assignments are made.
 The result of this change is a significant improvement in time complexity,
 reducing the original system from 
\begin_inset Formula $O(2n)$
\end_inset

 to 
\begin_inset Formula $O(n)$
\end_inset

, whereby 
\begin_inset Formula $n$
\end_inset

 is the number of objects within the given set of data.
 Put into practice, the results of experiments performed on this specialised
 algorithm are shown in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Results-and-Discussion"

\end_inset

Results and Discussion
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Details"

\end_inset

Experimental details
\end_layout

\begin_layout Standard
The experimentation detailed in this section was carried out within the
 
\emph on
Waikato Environment for Knowledge Analysis
\emph default
 (WEKA) suite for machine learning.
 This software, developed in the Java programming language, offers a powerful
 testing harness for analysis of various data mining concepts and implementation
s.
 In addition, it offers the ability to extend the suite with additional
 modules, and in the case of the presented work, clustering methods.
 
\end_layout

\begin_layout Standard
Building upon the functionality that already exists within WEKA, several
 existing techniques - specifically, DBSCAN and K-Medoids - were implemented,
 as were the customisations detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Customisations"

\end_inset

.
 Once developed, these clustering algorithms were applied to the specified
 test set of forest fire data to determine their effectiveness in terms
 of the number of clusters found and noise points classified.
 Efficiency was also monitored in terms of running time complexity.
 For each different input and clustering technique, the effects of the previousl
y-mentioned customisations were also noted, and corresponding results recorded.
 Analysis of these results aimed to demonstrate whether the modified algorithms
 were able to offer any degree of improvement over their original counterparts.
\end_layout

\begin_layout Standard
Each test was performed on the same computing hardware in order to ensure
 objectivity - a recent-model desktop computer with a 2.5GHz Intel Centrino
 dual-core processor and 3GB of RAM, running the latest stable release of
 the Java Virtual Machine (JVM 1.6).
 These experiments examined a range of varying input parameters to determine
 their effect on an algorithm's results, and to determine any changes in
 speed.
 
\end_layout

\begin_layout Standard
It should be noted, however, that whilst the results of the experiments
 described may appear conclusive, there are a number of other factors that
 may have a significant impact upon the generated output.
 For example, consider the performance measure of a given clustering technique.
 In order to perform this analysis, the algorithms need to be run on a computer
 which, due to operating system overhead, may skew results as a result of
 other tasks consuming processor load.
 Similarly, given that the algorithms detailed here are implemented using
 Java, the JVM or WEKA suite itself may implement some degree of caching,
 such that the results of one test may unknowingly become the first pre-processi
ng step for the next.
 These issues can be overcome, however, through the consideration of generalised
 trends and average results over a number of different tests.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Comparison-Results"

\end_inset

Comparison results
\end_layout

\begin_layout Subparagraph*
Density-based Performance
\end_layout

\begin_layout Standard
The four clustering algorithms used concern density-based techniques: the
 original DBSCAN algorithm that was implemented within WEKA, the newly implement
ed DBSCAN algorithm (
\emph on
UltraDBScan
\emph default
), its customised counterpart described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Customisations-DBSCAN"

\end_inset

 (
\emph on
UltraDBScan (Custom)
\emph default
), and finally, the WEKA implementation of the OPTICS method.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeed.png
	lyxscale 75
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-Performance-Results"

\end_inset

Density-based performance by epsilon
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeedminpoints.png
	lyxscale 75
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-Performance-Results-2"

\end_inset

Density-based performance by 
\emph on
minPoints
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement p
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeedattrib.png
	lyxscale 75
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-attrib"

\end_inset

Density-based performance by attributes
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-Performance-Results"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-Performance-Results-2"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-performance-attrib"

\end_inset

 show a detailed comparison of the performance of these density-based clustering
 algorithms.
 The first demonstrates the speed of clustering based upon different 
\begin_inset Formula $\varepsilon$
\end_inset

 distance values, given the expectation that a larger 
\begin_inset Formula $\varepsilon$
\end_inset

 value would result in generally larger clusters being found.
 The second graph shows that whilst there are minor fluctuations from the
 change in 
\emph on
minPoints, 
\emph default
the overall trend appears to be that as the value increases, the time taken
 to achieve results decreases.
 The exception to this appears to be the OPTICS algorithm, which was otherwise
 essentially the same throughout.
 The final figure examines the change in speed for each method given an
 increasing number of attributes that need to be processed.
\end_layout

\begin_layout Standard
These first set of tests demonstrate that for the given data set, the processing
 time is generally the same for each clustering algorithm, irrespective
 of a change in 
\begin_inset Formula $\varepsilon$
\end_inset

.
 Similarly, the second set of testing shows that the processing time for
 each technique increased at a similar rate as the number of attributes
 grew.
 The tests demonstrate that, on average, the original DBSCAN algorithm has
 the worst performance of each of the algorithms for increases in both 
\begin_inset Formula $\varepsilon$
\end_inset

 distance and the number of attributes present.
 Notably, the new implementation of UltraDBScan was able to typically perform
 faster than that of the original implementation that ships within WEKA.
 On closer inspection of the source code behind the original algorithm,
 it was clear that a difference in how each traverses to a given point's
 neighbours was the cause of the performance improvement.
 The two remaining techniques, the customised UltraDBScan and OPTICS, were
 able to outperform the rest.
 The reasoning for the customised algorithm performing better than its original
 counterpart is the addition of the dynamic 
\begin_inset Formula $\varepsilon$
\end_inset

 value.
 As this value essentially constricts the allowable spread of clusters,
 fewer recursive calls are required to the objects surrounding each cluster.
 This reduction in processing complexity produces an algorithm that is typically
 faster, overall.
 
\end_layout

\begin_layout Subparagraph*
Partitioning-based Performance 
\end_layout

\begin_layout Standard
The three clustering algorithms used for these tests focus on partitioning-based
 techniques: the K-Means algorithm, which ships as a standard algorithm
 within WEKA, the newly implemented K-Medoids algorithm, and its customised
 counterpart - described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Customisations-K-Medoids"

\end_inset

, referred to as 
\emph on
K-Medoids (Custom).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoidsspeed.png
	lyxscale 75
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Partitioning-performance-k"

\end_inset

Partitioning performance by 
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoidspeedattrib.png
	lyxscale 75
	width 85col%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Partitioning-performance-attrib"

\end_inset

Partitioning performance by attributes
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Partitioning-performance-k"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Partitioning-performance-attrib"

\end_inset

 show a performance analysis of these partitioning-based clustering algorithms.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Partitioning-performance-k"

\end_inset

 shows that changing the number of clusters to be defined ( 
\emph on
k
\emph default
) effects the performance of each method; as expected given that a larger
 value of 
\emph on
k
\emph default
 will result in more clustering calculations being performed.
 In order to test how well each clusterer handles a higher level of dimensionali
ty, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Partitioning-performance-attrib"

\end_inset

 shows the test results from changing the number of attributes of each data
 object processed.
\end_layout

\begin_layout Standard
The first set of tests highlight that as the value of 
\emph on
k
\emph default
 increases and more clusters are calculated, the time needed to process
 the data set similarly increases.
 It is evident that the K-Medoids algorithm is the worst-performing of those
 tested, with the time taken growing extremely quickly for higher values
 of 
\emph on
k.

\emph default
 This time complexity can be attributed to the number of calculations that
 need to take place for each cluster examined.
 Thus, these results are to be expected from the standard K-Medoids algorithm.
 However, the modified version of K-Medoids was shown to be the most robust.
 This demonstrates that the changes suggested have a notable effect on processin
g time, given that far fewer recalculations are required for a given data
 set.
 Finally, the K-Means algorithm ranked between the other two methods, and
 showed a gradual increase in processing time across testing.
 Most notably, this rate change was less than that of K-Medoids, but evidently
 greater than that of the customised algorithm.
\end_layout

\begin_layout Standard
Trends in the results when changing the number of attributes of each data
 object differed somewhat from that of modifying the value of 
\emph on
k
\emph default
.
 In these tests, the two flavours of K-Medoids algorithms performed equally
 well, with the customised version, as expected, slightly out-performing
 that of its original version.
 The K-Means algorithm was slower, given the change in parameters.
 It should be expected that each of these algorithms suffers a decrease
 in performance as the dimensionality of the given data objects becomes
 higher, and this is evident in the results shown.
 The increase in the time taken for the K-Means algorithm to complete most
 probably relates to a difference in implementation, rather than a fundamental
 change in the way each algorithm handles attributes and distances.
\end_layout

\begin_layout Subparagraph*
Clustering Results
\end_layout

\begin_layout Standard
The ability for each of the density-based algorithms to detect noise was
 also examined.
 The results for each of the density algorithms are described within Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-performance-noise"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-performance-noise-2"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscannoise.png
	lyxscale 75
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-noise"

\end_inset

Density-based performance by resulting noise from change in epsilon
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanminpoints.png
	lyxscale 75
	width 85col%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-noise-2"

\end_inset

Density-based performance by resulting noise from change in 
\emph on
minPoints
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The overall trend described by these tests demonstrates the expected behaviour
 of any density-based technique: as the distance checked for neighbours
 increases, the number of points classified as noise decreases.
 In addition, this directly affects the size of the clusters discovered.
 Similarly, as the value of 
\emph on
minPoints
\emph default
 increases, the number of points being classified as noise increases also.
\end_layout

\begin_layout Standard
The results demonstrate that all techniques - with the exception of OPTICS
 which considered all points to be noise for all values of 
\begin_inset Formula $\varepsilon$
\end_inset

 - were able to detect similar amounts of noise, and thus define a similar
 number of clusters.
 The exception to this trend was the customised DBSCAN algorithm which,
 for most 
\begin_inset Formula $\varepsilon$
\end_inset

 distances, and values of 
\emph on
minPoints
\emph default
, tested, discovered a slightly greater amount of noise, or a greater number
 of clusters.
 The reasoning for these changes is directly associated with the dynamic
 change of 
\begin_inset Formula $\varepsilon$
\end_inset

, as the size of clusters grew throughout the processing of the database.
 Having this modification present also demonstrated that the number of points
 assigned to certain clusters changed also.
 For instance, with a 
\emph on
minPoints
\emph default
 value of 1, the size of several clusters changed considerably, thus illustratin
g the the dynamic 
\begin_inset Formula $\varepsilon$
\end_inset

 value is having an effect.
 In comparison, the same cluster was defined as only a single point for
 the other methods.
 Thus, these results demonstrate that, depending on the application, this
 type of interchangeable variable may bring about more-suitable clustering
 assignments.
\end_layout

\begin_layout Standard
Overall, the original WEKA implementation of DBSCAN performed comparably
 with the newly implemented UltraDBScan for all tests.
 This result demonstrates that not only was the new implementation able
 to compute clusters correctly, as it was able to find the same amounts
 of noise and clusters, but that the provided algorithm was not as efficiently
 written.
 
\end_layout

\begin_layout Standard
Due to comparable limitations of the K-Medoids and K-Means algorithms, a
 similar figure cannot be produced.
 The inherent purpose of these partition-based techniques is to ensure that
 all elements in a database are assigned to a cluster, and thus none are
 discarded as noise.
 This is described in the operations in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Medoids"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Means"

\end_inset

, whereby each object present is assigned to its nearest medoid or centroid.
 Unlike the density-based techniques described here, no degree of analysis
 occurs to determine if a given point should legitimately belong to any
 cluster.
 
\end_layout

\begin_layout Subsection
Issues
\end_layout

\begin_layout Standard
As is to be expected of any data mining procedure, the best results from
 a clustering operation depend entirely upon many different factors.
 These include, but are not limited to, the actual data set being processed,
 the clustering technique selected, and the input parameters provided to
 the technique.
 In some situations, a certain algorithm may determine suitable clusters
 within one data set, but may be completely ineffective with another.
 Likewise, using unsuitable input parameters for a given clustering process
 may result in similarly unsuitable cluster assignments for data instances.
 As a result, all of the aforementioned aspects of data mining need to be
 considered before an algorithm is chosen, and the application of the technique
 should be performed by a person who has sufficient domain knowledge to
 make correct decisions that will reveal suitable output.
\end_layout

\begin_layout Standard
Furthermore, as detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

, the most suitable types of data are detailed for each given subset of
 clustering techniques.
 It should be noted that the results demonstrated in this paper summarise
 tests executed within the given testing environment, and only represent
 output associated with the single data set of forest fire information.
 described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Data-Set"

\end_inset

.
 As mentioned previously, using a different set of data may, and most likely
 will, reveal different patterns and see a change in processing time complexity.
 This would be especially evident if incorrect input parameters are used
 for the given clustering algorithms.
 However, the results detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Comparison-Results"

\end_inset

 indicate that for the data set under consideration, the DBSCAN and K-Medoids
 algorithms (and their customised counterparts) reveal useful clusters.
 From this, it can be generally inferred that for similar types of data,
 these same algorithms should result in the definition of similar clusters
 in a similar processing time-frame.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Conclusion"

\end_inset

Conclusion
\end_layout

\begin_layout Standard
The results demonstrate that for density-based algorithms, an incorrect
 or non-optimised selection of 
\begin_inset Formula $\varepsilon$
\end_inset

 distance can significantly alter the clusters defined.
 However, a change in 
\begin_inset Formula $\varepsilon$
\end_inset

 distance had very little effect on the efficiency of the technique.
 This outcome is caused by the way in which this type of clusterer progresses
 through the database.
 Whilst some algorithms may utilise recursion to examine points, the time
 complexity essentially resolves as 
\begin_inset Formula $O(n\log n)$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of objects in the database.
 This occurs because each point only needs to be checked once, and only
 compared to its own neighbours.
 Thus, any effect caused by either of the input variables, 
\emph on
minPoints
\emph default
 or 
\begin_inset Formula $\varepsilon$
\end_inset

, will be negligible.
 This stands in contrast to the change in performance seen when the 
\emph on
k
\emph default
 value is altered for partitioning-based clustering techniques.
 As this value directly affects the number of calculations that need be
 performed on each data object, searching for increasing numbers of clusters
 has an inflationary effect on the time required to process a data set.
\end_layout

\begin_layout Standard
A notable conclusion that can be drawn from the results is that an increase
 in the number of attributes being processed will see a decrease in the
 processing speed for all given clustering types tested.
 The cause of reduction in performance is related to the time taken to compute
 distances between each of the objects that need to be compared.
 In lower dimensions, when few attributes are present, calculations are
 simple as the results have shown.
 However, as higher dimensions need to be considered, the time complexity
 increases accordingly.
\end_layout

\begin_layout Standard
The given data set demonstrates potential for both of the customised algorithms.
 For UltraDBScan, the set showed that different quantities of noise could
 be found.
 Whilst not strictly applicable to this specific data set, this ability
 to locate additional noise could greatly affect the clustering results
 for data from other domains.
 As an example, consider several clusters with a 
\emph on
bridge
\emph default
 of objects between them.
 As the algorithm begins to cross the bridge from an initial cluster, it
 will decrease the distance it checks for neighbours accordingly, essentially
 severing the link, and declaring each point of the bridge as noise.
 
\end_layout

\begin_layout Standard
Whilst not quantifiable, due to the difficulty of attempting to compare
 one cluster to another, the improvement suggested in this paper to K-Medoids
 is similarly useful.
 As this customised method no longer needs to re-analyse the entire data
 set twice for each iteration, the time complexity decreases significantly,
 and results in a technique that is far more robust as an increasing number
 of clusters is found.
\end_layout

\begin_layout Standard
Overall, the different methods of clustering have been shown to produce
 varying levels of performance and yield different degrees of usefulness
 from results.
 Whilst there are similarities between the clustering algorithms and their
 generalised types, the overall results produced show that a choice of technique
 is one of the most important decisions to be made in clustering.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Future-Work"

\end_inset

Future Work
\end_layout

\begin_layout Standard
As data mining, and specifically clustering, becomes more of a part of every-day
 business and organisational operation, the necessity for faster and equally
 accurate algorithms rises.
 The improvements suggested in this paper, while novel and logically presented,
 are not entirely comprehensive and thus scope for further ongoing research
 exists in several areas.
 There are many other potential improvements that could be investigated
 within the field of clustering techniques - extensions to the work presented
 here are essentially unbounded.
 
\end_layout

\begin_layout Standard
Partitioning techniques (including the suggested UltraK-Medoids algorithm)
 require that all points in the data set are allocated to a cluster, unlike
 density-based methods which can leave noise points in an unclustered state.
 This creates varying levels of noise sensitivity across the K-algorithms,
 and research into development of a method that works on both principles
 would be beneficial.
 
\end_layout

\begin_layout Standard
The requirement that both partitioning and density-based techniques specify
 input parameters places certain limitations on the effectiveness of the
 algorithms when these values are not optimised.
 Choosing the values correctly takes both experimentation and time, which
 adds overhead to the process of drawing useful conclusions from mined data.
 Partitioning methods require that the number of clusters to be found is
 statically set, and incorrect choice of this value can lead to serious
 noise sensitivity.
 Investigation of the advantages of allowing the algorithm to dynamically
 choose the number of clusters (as used in density-based techniques) is
 thus justified.
 Density-based methods suffer from incorrect choice of 
\begin_inset Formula $\varepsilon$
\end_inset

 neighbourhood radius and minimum points per cluster.
 UltraDBScan has addressed the issue of 
\begin_inset Formula $\varepsilon$
\end_inset

 selection by allowing the algorithm to dynamically chose this value as
 it progresses through points in the data set under analysis.
 However, more research into managing the effect of the minimum points variable
 would be clearly beneficial.
 By reducing or eliminating the effects these variables have on results,
 a system will be able to produce more objective output with greater accuracy.
\end_layout

\begin_layout Standard
Given that the algorithms this paper discusses have been in existence for
 up to 20 years, and that there have been various papers and theses produced
 regarding the aforementioned issues, the suggestions made in this section
 appear as the logical path for future investigation.
 Others can extend this work by seeking alternative ways of improving the
 semantics of the proposed algorithms, rather than attempting to improve
 the actual operations which have been already been fine tuned through the
 extensive research summaries in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

.
 
\end_layout

\begin_layout Section
\start_of_appendix
Implementation
\end_layout

\begin_layout Standard
This paper presents a method for developing for classes for WEKA, using
 a Java integrated development environment (IDE) named IntelliJ.
 Developed by a company named JetBrains, IntelliJ has been found to be one
 of the most powerful and useful development environments, and as such,
 these instructions are specific to this program.
 They may be extended, with some investigation, to other development environment
s, such as NetBeans.
 However, no assurances can be placed upon whether the same procedure here
 can be replicated using another program.
\end_layout

\begin_layout Standard
Follow these guidelines to easily create and further develop clustering
 algorithms, classifiers and association rules processes for use within
 WEKA:
\end_layout

\begin_layout Enumerate
Install the latest version of IntelliJ from 
\family typewriter
http://www.jetbrains.com/idea/
\family default
.
 It is a commercially licensed piece of software, so a developer either
 needs to use the trial version, or otherwise purchase or obtain a license
 key.
 You will also need a Java Development Kit (JDK); version 1.6 is the latest
 as of the time of writing.
\end_layout

\begin_layout Enumerate
Install the latest version of WEKA from 
\family typewriter
http://www.cs.waikato.ac.nz/ml/weka/
\family default
.
 
\end_layout

\begin_layout Enumerate
Create a new IntelliJ project, and set up your JDK accordingly.
\end_layout

\begin_layout Enumerate
Set up your project so that you can use the WEKA classes within your own.
\end_layout

\begin_deeper
\begin_layout Enumerate
Go to File 
\begin_inset Formula $\Rightarrow$
\end_inset

 Settings 
\begin_inset Formula $\Rightarrow$
\end_inset

 Project Settings 
\begin_inset Formula $\Rightarrow$
\end_inset

 Global Libraries
\end_layout

\begin_layout Enumerate
Add a new 
\emph on
Global Library 
\emph default
and call it something like 
\begin_inset Quotes eld
\end_inset

WEKA
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Enumerate
Attach WEKA's JAR file by selecting 
\begin_inset Quotes eld
\end_inset

Attach Classes
\begin_inset Quotes erd
\end_inset

 and locating the relevant file.
 You will find this located in your WEKA install location.
 You may wish to make a copy or move this JAR into your own project folder.
\end_layout

\begin_layout Enumerate
Click onto 
\begin_inset Quotes eld
\end_inset

Apply
\begin_inset Quotes erd
\end_inset

 to save the changes.
 The library will be parsed accordingly and you can now access and utilise
 WEKA libraries in your own code.
\end_layout

\end_deeper
\begin_layout Enumerate
Add a run configuration for WEKA, so you can run and debug your code from
 IntelliJ.
\end_layout

\begin_deeper
\begin_layout Enumerate
Go to Run 
\begin_inset Formula $\Rightarrow$
\end_inset

 Edit Configurations
\end_layout

\begin_layout Enumerate
Click onto the 
\begin_inset Quotes eld
\end_inset

Add
\begin_inset Quotes erd
\end_inset

 button, and select 
\begin_inset Quotes eld
\end_inset

Application
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Enumerate
Under 
\begin_inset Quotes eld
\end_inset

Main class
\begin_inset Quotes erd
\end_inset

 enter 
\family typewriter
weka.gui.Main
\end_layout

\begin_layout Enumerate
Under 
\begin_inset Quotes eld
\end_inset

VM Parameters
\begin_inset Quotes erd
\end_inset

 you can control the JVM environment.
 Using a value such as 
\family typewriter
-Xmx512m
\family default
 will allocate the JVM 512MB of RAM, rather than 128MB or less that would
 otherwise be the default.
 Tweaking these settings is especially useful if you find your data set
 is too complex and it causes the JVM to run out of memory.
\end_layout

\begin_layout Enumerate
You may wish to change your 
\begin_inset Quotes eld
\end_inset

Working directory
\begin_inset Quotes erd
\end_inset

 to be where your data sets reside, for ease of access.
 This location specifies where the default folder is for when you open any
 file dialogs within WEKA.
\end_layout

\begin_layout Enumerate
Ensure that your 
\emph on
classpath
\emph default
 is specified to be that of your current project so your classes can be
 found within WEKA.
\end_layout

\begin_layout Enumerate
Ensure that under 
\begin_inset Quotes eld
\end_inset

Before launch
\begin_inset Quotes erd
\end_inset

 the option 
\begin_inset Quotes eld
\end_inset

Make
\begin_inset Quotes erd
\end_inset

 is 
\emph on
not 
\emph default
ticked.
 If it is, your classes will not be visible as your build location will
 be inadvertently cleaned when you run WEKA.
\end_layout

\begin_layout Enumerate
Click 
\begin_inset Quotes eld
\end_inset

OK
\begin_inset Quotes erd
\end_inset

 to save the changes.
\end_layout

\end_deeper
\begin_layout Enumerate
Make your classes visible within WEKA requires an extra step, as classes
 are found dynamically in the latest version of the data mining suite.
\end_layout

\begin_deeper
\begin_layout Enumerate
Locate the previously-used JAR file for the WEKA library - it is most likely
 in your WEKA install location.
\end_layout

\begin_layout Enumerate
Open it within your favourite archive extraction utility, noting that you
 may need to rename the 
\family typewriter
.jar
\family default
 to 
\family typewriter
.zip
\family default
, if your program does not support the 
\family typewriter
.jar
\family default
 format.
\end_layout

\begin_layout Enumerate
Enter the 
\begin_inset Quotes eld
\end_inset

weka
\begin_inset Quotes erd
\end_inset

 folder, and then the 
\begin_inset Quotes eld
\end_inset

gui
\begin_inset Quotes erd
\end_inset

 folder.
\end_layout

\begin_layout Enumerate
Find the 
\family typewriter
GenericPropertiesCreator.props
\family default
 file and extract this into the base of your user home directory.
\end_layout

\begin_layout Enumerate
Edit the file in a text editor, and near the top, locate the line 
\family typewriter
UseDynamic=false
\family default
.
 
\end_layout

\begin_layout Enumerate
Change the 
\family typewriter
false
\family default
 to 
\family typewriter
true
\family default
 and your classes, once created correctly, will be accessible from within
 WEKA.
\end_layout

\end_deeper
\begin_layout Enumerate
Build your classes, taking the following into account:
\end_layout

\begin_deeper
\begin_layout Enumerate
Make each of your classes part of the 
\family typewriter
weka
\family default
 package.
 For example, if you were creating a clusterer, like the ones described
 in this paper, use the 
\family typewriter
weka.clusterers
\family default
 package.
\end_layout

\begin_layout Enumerate
Make each of your classes extend a certain interface or abstract class from
 WEKA.
 This is necessary to allow WEKA to dynamically find your classes, and IntelliJ
 will automatically fill out which methods need to be implemented and extended.
 For example, you can use DensityBasedClusterer for this type of clusterer,
 or RandomisableClusterer for those that need some degree of randomisation
 in their processing.
\end_layout

\end_deeper
\begin_layout Enumerate
When you want to test your classes, go to Build 
\begin_inset Formula $\Rightarrow$
\end_inset

 Rebuild Project.
 Your classes will be compiled in your build directory.
 Then, run the WEKA configuration from IntelliJ.
 WEKA will start, and your new classes can be tested.
\end_layout

\begin_layout Standard
This introduction has introduced the method of creating new classes for
 the WEKA data mining environment.
 Further documentation is available, as at the time of writing, from 
\family typewriter
http://weka.wiki.sourceforge.net/
\family default
.
 The various resources at this website explain details at even more depth
 on how each part of your code is used in WEKA.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "clustersdk"
options "latex8"

\end_inset


\end_layout

\end_body
\end_document
