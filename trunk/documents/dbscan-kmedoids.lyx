#LyX 1.6.0rc3 created this file. For more info see http://www.lyx.org/
\lyxformat 340
\begin_document
\begin_header
\textclass latex8
\begin_preamble

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%


\usepackage{latex8}

\usepackage{times}

\usepackage{epsf}

\usepackage{epsfig}

\usepackage[config, font={sf,bf}]{caption}

\usepackage[config, font={small, sf}]{subfig}

\usepackage{latexsym}
   % additional packages that may be read in
    % to augment generic LaTeX; needed for \mathbb

\newcommand{\VD}{{Voronoi diagram}}
\newcommand{\VDs}{{Voronoi diagrams}}
%\documentstyle[times,art10,twocolumn,latex8]{article}

%-------------------------------------------------------------------------
% take the % away on next line to produce the final camera-ready version


%-------------------------------------------------------------------------
\end_preamble
\options times
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle empty
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Clusterers: a Comparison of Partitioning and Density-Based Algorithms and
 a Discussion of Optimisations
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
author{David Breitkreutz
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{
\backslash
& Information Technology} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia
\backslash

\backslash
 David.Breitkreutz@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
and
\end_layout

\begin_layout Plain Layout

Kate Casey
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit {
\backslash
& Information Technology} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Kate.Casey@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Though data mining is a relatively recent innovation, the improvements it
 offers over traditional data analysis have seen the field expand rapidly.
 Given the critical requirement for the efficient and accurate delivery
 of useful information in today's data-rich climate, significant research
 in the topic continues.
\end_layout

\begin_layout Abstract
Clustering is one of the fundamental techniques adopted by data mining tools
 across a range of applications.
 It provides several algorithms that can assess large data sets based on
 specific parameters and group related data points.
\end_layout

\begin_layout Abstract
This paper compares two widely used clustering algorithms, K-Medoids and
 Density-Based Spatial Clustering of Applications with Noise (DBSCAN), against
 other well-known techniques.
 The initial testing conducted on each technique utilises the standard implement
ation of each algorithm.
 Further experimental work tests modifications to these methods in order
 to investigate potential improvements of results or efficiency.
 Various key applications of clustering methods are detailed, and several
 areas of future work have been suggested.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Introduction"

\end_inset

Introduction 
\end_layout

\begin_layout Standard
Clustering is promoted as an extremely powerful means of grouping related
 data points, and can efficiently reveal highly relevant trends in a source
 data set.
 These capabilities extend to large sets of data, and are applicable to
 stores of information used by scientists, researchers, and businesses.
 As a result, the field has developed into one of the foremost research
 areas in modern computing.
 These data mining techniques can analyse both large-scale databases and
 data warehouses.
 There exist numerous algorithms for performing such analysis, and each
 may be more suitable to certain circumstances, depending entirely upon
 domain-specific parameters.
 As such, the efficiency and accuracy of results from these data mining
 tasks relies directly upon the choice of a suitable algorithms.
 Thus, for as many different types of data set that exist, there is the
 requirement for further research into the improvements of these techniques.
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Data mining in general has a multitude of applications across a wide variety
 of fields.
 Pattern recognition for image analysis, medical diagnostics, sales forcasting
 and weather prediction are a recognised as a few of the more traditional
 usages.
 However, due to extensive development in the field and the recent explosion
 in data recording the capabilities extend far beyond these basic functions.
 Onboard computer analysis in vehicles, product quality analysis, targeted
 advertising campaigns, spam email filtration, fraud detection, and online
 crime analysis are but a few of the fields into which data mining now extends.
 
\begin_inset CommandInset citation
LatexCommand cite
key "john1999bsd"

\end_inset

 Clustering applies to all of the aforementioned applications as a subset
 of data mining.
 
\end_layout

\begin_layout Standard
A prominent example of this functionality is the application of emergency
 situation data analysis; for example, data recorded by authorities during
 forest fires.
 This paper will investigate that specific scenario in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Details"

\end_inset

, with the test sets utilised containing fire data from Portugal
\emph on
.
\end_layout

\begin_layout Subsection
Other Methods
\end_layout

\begin_layout Standard
There are a wide variety of clustering methodologies that exist within this
 field.
 These include, but are not limited to, grid-based categorisation, density-based
 grouping, hierarchical segmentation, and constraint-based analysis.
 As these techniques have been already widely researched, there are many
 existing works that compare and constrast each.
 The interested reader is directed to 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 for further information regarding these topics.
\end_layout

\begin_layout Standard
The research outlined in this paper is concerned with the application of
 two of the most widely used methods; 
\emph on
partitioning-based 
\emph default
and 
\emph on
density-based
\emph default
 clustering.
\end_layout

\begin_layout Standard
Partitioning algorithms are effective for mining data sets when computation
 of a dendrogram (clustering tree) representation is infeasible.
 
\begin_inset CommandInset citation
LatexCommand cite
key "jain1999dcr"

\end_inset

 In particular, K-Medoids is highly efficient for moderately sized data
 sets with spherical-type clusters.
 However, due to the inherent swapping of medoids to optimise the clustered
 solution, the algorithm suffers greatly as large data sets are introduced.
 
\end_layout

\begin_layout Standard
Density-based algorithms perform optimally when operating upon spatially-indexed
 data.
 The methods provide benefits when analysing data sets that contain high
 levels of noise or when clusters are arbitrarily shaped.
 Specifically, DBSCAN is able to grow clusters whilst the MinPts threshold
 is not satisfied within the a specified neighbourhood, thus efficiently
 dividing real data and noise in a variety of shapes.
\end_layout

\begin_layout Subsection
Purpose of Paper
\end_layout

\begin_layout Standard
This paper focuses upon the DBSCAN and K-Medoids algorithms from the density-bas
ed and partitioning families respectively.
 It outlines the design, advantages and disadvantages of each method, and
 studies similar methodologies.
 The work contained goes on to propose both basic implementations of the
 techniques, as well as initiating new research into improving the efficiency
 of the methods.
 A great deal of literature already exists regarding these techniques, and
 a review of these documents can be found in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Data-Set"

\end_inset

Experimental Data Set
\end_layout

\begin_layout Standard
The data set chosen for the comparison of and ammendments to the K-Medoids
 and DBSCAN techniques is 
\emph on
forest fires
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "cortez:dma"

\end_inset

.
 The set contains environmental data from fire instances in northeastern
 Portugal, and represents instances within space, and includes attributes
 such as the date, temperature, humidity, wind conditions, rain information
 and burned area.
 
\end_layout

\begin_layout Standard
Analysing this data set is necessary because forest fires cause millions
 of dollars damage to property and claim many lives each year.
 If clustering techniques can be used to more accurately determine the patterns
 of such fires given provailing environmental conditions, scientists and
 researchers will be able to achieve further understanding of the phenomenon.
 This knowledge will reap enormous benefits; predictive techniques can be
 updated, analysis centres local to fires can more accurately forcast fire
 behaviour, government agencies can be continuously informed of these conclusion
s, and residents and business owners can be warned earlier.
 This improved system will result in reduced property damage, lower resoration
 costs and, ultimately, fewer lives lost.
\end_layout

\begin_layout Subsection
Results Overview
\end_layout

\begin_layout Standard
The results explained in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

 form the basis of the experimental portion of this paper.
 Based on the forest fires data set, these figures reflect the implementation
 results of the DBSCAN and K-Medoids algorithms.
 
\end_layout

\begin_layout Standard
In addition, testing of the improvements that this paper suggests for both
 the aforementioned techniques has shown a level of computational improvement
 under both methods for the data set utilised.
 These findings are detailed and discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Related-Work"

\end_inset

Related Work
\end_layout

\begin_layout Standard

\emph on
need to write about the other types of density and partiioning that have
 been tried other than dbscan and kmedoids
\end_layout

\begin_layout Subsection
Data Analysis
\end_layout

\begin_layout Standard
A formal definition by Moore 
\begin_inset CommandInset citation
LatexCommand cite
key "moore1998pas"

\end_inset

 states that data analysis is 
\emph on

\begin_inset Quotes eld
\end_inset

the examination of data for interesting patterns and striking deviations
 from those patterns
\begin_inset Quotes erd
\end_inset


\emph default
.
 Such processing can lead to conclusions which assist a multitude of people
 in varying tasks; from medical diagnostics to population growth predictions
 to forest fire forcasting.
 The potential developments in these research areas due to quality data
 analysis are not only beneficial in terms of further knowledge and understandin
g, but will inevitably benefit ordinary people in untold ways far beyond
 the scope of this paper.
 Data mining is a subset of the broader data analysis field, and as such
 can provide the advantages previously discussed when large data sets require
 processing.
\end_layout

\begin_layout Subsection
Clustering
\end_layout

\begin_layout Standard
Aggarwal et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "aggarwal1999fap"

\end_inset

 define the process of clustering as:
\emph on
 
\begin_inset Quotes eld
\end_inset

Given a set of points in multidimensional space, find a partition of the
 points into clusters so that the points within each cluster are close to
 one another
\begin_inset Quotes erd
\end_inset


\emph default
.
 Proximity is measured using a variety of algorithm-specific metrics, such
 that the closer two arbitrary points are to one another, the more strongly
 they are considered to be related.
 This process results in defined groupings of similar points, where strong
 inter-cluster and weak intra-cluster relationships exist between points,
 an example of which is detailed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-clustered"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename clustering.png
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-clustered"

\end_inset

Example of clustered points
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clustering can be categorised in machine learning terms as a form of 
\emph on
unsupervised learning
\emph default
; that is, clusters are representative of hidden patterns in the source
 data set
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

.
 Raw information is analysed and relationships are discovered by the algorithm
 without external direction or interference; learning through observation
 rather than by the study of examples
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 In addition, this objectivity translates to an effective means of data
 analysis without the opportunity for subjective human conclusions to be
 drawn from data.
\end_layout

\begin_layout Subsection
Partitioning Clustering Algorithms
\end_layout

\begin_layout Standard
Partitioning methods define clusters by grouping data points into 
\emph on
k
\emph default
 partitions (defined at runtime).
 A point is determined to be 
\emph on
similar
\emph default
 to other points within its partition, and 
\emph on
dissimilar
\emph default
 to points that lie outside the boundary of that partition 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 Comparison is based on the characteristics of the data set provided.
 Thus, the algorithms rely on the conversion of semantic data attributes
 (width, height, shape, colour, cost or others) into points that determine
 physical location on a set of mathematical axes.
 This provides an objective and computationally acceptable framework for
 analysis.
 In the simplest case only two attributes exist, and thus the conversion
 renders a point on a standard Cartesian plane.
 This process is greatly complicated when, as often occurs in highly detailed
 source sets, hundreds of attributes are present.
 The rendering plane takes on high dimensionality, and the complexity of
 analysis becomes very computationally expensive.
 
\end_layout

\begin_layout Standard
Partitioning offers several features, including efficiency in processing
 spherical clusters in small to medium data sets, and scalability across
 smaller applications.
 However, the algorithms suffer from extreme complexity and computational
 cost at higher dimensions in large sets, require the user to specify the
 number of clusters to be determined (
\emph on
k
\emph default
) and experience problems defining arbitrarily shaped clusters.
\end_layout

\begin_layout Standard
Previous work has outlined several clustering methods within the partitioning
 family; 
\emph on
K-means
\emph default
 converts data to numerical points on a set of axes and calculates the numerical
 mean of these points to dictate cluster centroids 
\begin_inset CommandInset citation
LatexCommand cite
key "hartigan1979kmc"

\end_inset

, 
\emph on
K-modes 
\emph default
is similar to K-means though it calculates the mode of point groupings instead
 of the mean to define cluster centroids , and 
\emph on
K-medoids
\emph default
 converts data points to objects on a set of axes and arbitrarily chooses
 different objects to act as cluster centroids until the clusters are optimised
 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1987cmm"

\end_inset

.
 As K-Medoids provides dramatic improvements in noise immunity a great deal
 of further research has been pursued, including the forthcoming work in
 this paper.
 
\emph on
PAM (Partitioning Around Medoids)
\emph default
 randomly chooses medoids, then randomly calculates the result of swapping
 these with other random objects in an attempt to improve the clustering
 outcome 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1990pam"

\end_inset

.
 This approach works adequately for small source sets, but does not scale
 well as complexity at each iteration is quadratic.
 
\emph on
CLARA (Clustering LARge Applications) 
\emph default
takes a single random selection of points in the set as representative data
 and then applies PAM to this smaller subset 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1990fgd"

\end_inset

.
 Reducing the size of the set for analysis improves scalability as complexity
 at each iterations is linear, but the method suffers depending on the quality
 of sample data chosen.
 
\emph on
CLARANS (Clustering Large Applications based on RANdomized Search) 
\emph default
takes a random dynamic selection of data at each step of process – thus
 the same sample set is not used throughout the clustering process.
 As a result, better randomisation of source data is achieved, but the method
 still suffers depending on the selection and can be very slow as the algorithm
 is approximately quadratic
\emph on
 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "ng1994eae"

\end_inset

.
 UltraK-Medoids, the algorithm proposed in this paper, aims to bla bla bla..............
\end_layout

\begin_layout Subsection
Density-Based Clustering Algorithms
\end_layout

\begin_layout Standard
The density-based group of clustering algorithms represent a data set in
 the same manner as partitioning methods; converting an instance to a point
 using the data attributes of the source set.
 The plane contains clusters with high internal density and low external
 density in a similar manner to its partitioning ancestor 
\begin_inset CommandInset citation
LatexCommand cite
key "Kolatch01clusteringalgorithms"

\end_inset

.
 The process of adding points to a cluster is iterative, unlike partitioning
 methods.
 Nearest neighbours of each point can thus be investigated, arbitrary shapes
 formed, and existing clusters merged as the algorithm moves through all
 points.
 As a result, analysis can easily isolate noise instances from relevant
 data, whilst being able to cluster data object sets that include hollow
 formations.
 
\end_layout

\begin_layout Standard
Density-based algorithms provide advantages over other methods through their
 noise handling capabilities and ability to determine clusters with arbitrary
 shapes (eg.
 a hollow circle, torus or other non-convex formation).
 As with partitioning techniques, computational cost is a disadvantage when
 the technique is used with large amounts of source data and sets containing
 excessive noise.
 
\end_layout

\begin_layout Standard
Literature exists on several techniques that utilise the density-based clusterin
g concept.
 DENCLUE 
\begin_inset CommandInset citation
LatexCommand cite
key "hinneburg1998eac"

\end_inset

 and DBSCAN 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

.
 OPTICS 
\begin_inset CommandInset citation
LatexCommand cite
key "ankerst1999oop"

\end_inset

, GDBSCAN etc.
\end_layout

\begin_layout Standard
UltraDBSCAN, the algorithm proposed in this paper, aims to bla bla bla..............
\end_layout

\begin_layout Subsection
Alternative Clustering Methods
\end_layout

\begin_layout Standard
Two of the most popular clustering methods in use other than density-based
 and partitioning
\emph on
 
\emph default
are 
\emph on
heirarchical 
\emph default
and 
\emph on
grid-based
\emph default
.
\end_layout

\begin_layout Standard
Heirarchical algorithms can define clusters in two ways; either top-down
 (divisive) or bottom-up (
\emph on
agglomerative
\emph default
).
 Divisive techniques define a broader cluster first which encompasses all
 points, then split this into more specialised child clusters.
 These child clusters in turn spawn their own child clusters and the process
 continues until all points are accounted for.
 The agglomerative technique is the reverse, defining the smallest and lowest-le
vel clusters first and working upward until the entire set is contained
 in one cluster 
\begin_inset CommandInset citation
LatexCommand cite
key "grabmeier2002tca"

\end_inset

.
 The methods are advantageous in terms of processing time efficiency, but
 accuracy suffers as they do not allow for incorrect clustering decisions
 to be reversed (back-traversal).
 Chameleon 
\begin_inset CommandInset citation
LatexCommand cite
key "karypis1999chc"

\end_inset

 and BIRCH 
\begin_inset CommandInset citation
LatexCommand cite
key "zhang1997bnd"

\end_inset

 are examples of hierarchical methods; Chameleon concerns analysis of interobjec
t relationships at each hierarchical level and BIRCH utilises iterative
 relocation.
\end_layout

\begin_layout Standard
Grid-based techniques divide the data space into a finite number of cells
 (hence 
\emph on
grid
\emph default
) and clustering methods are applied to these segments.
 Due to this division of study space, such algorithms are dependent in terms
 of processing only on the quantity of cells at any dimension.
 Thus the technique has very high computational processing efficiency 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 The WaveCluster 
\begin_inset CommandInset citation
LatexCommand cite
key "sheikholeslami1998wmr"

\end_inset

 and STING 
\begin_inset CommandInset citation
LatexCommand cite
key "wang1997ssi"

\end_inset

 algorithms both employ grid-based clustering techniques; WaveCluster utilises
 wavelet transformations on source data and STING exhibits typical grid-based
 analysis behaviour.
\end_layout

\begin_layout Subsection
Alternative Analysis Techniques
\end_layout

\begin_layout Standard
Clustering differs from other classical data mining methodologies in a variety
 of ways and extensive listerature already exists explaining this.
 For the purpose of conciseness, this paper will compare and contrast clustering
 only with the techniques of 
\emph on
classification
\emph default
 and 
\emph on
prediction
\emph default
.
\end_layout

\begin_layout Standard
Classification is the process of constructing a learning model, or 
\emph on
classifier
\emph default
, that assigns points in a dataset to predefined classes
\begin_inset CommandInset citation
LatexCommand cite
key "fayyad1996dma"

\end_inset

.
 Whilst this definition initially appears similar to that of clustering
 in that points are grouped based on attributes, any similarities are thus
 exhausted.
 The technique utilises 
\emph on
supervised learning 
\emph default
as opposed to the unsupervised learning environment of clustering.
 The model is compiled by studying provided data (known as a training set)
 containing class labels; hence the technique is 
\begin_inset Quotes eld
\end_inset

supervised
\begin_inset Quotes erd
\end_inset

 due to example data exposure.
 Once the model is complete it can be applied to real, unlabelled data and
 assign each new tuple to its respective class
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 Classification is used in the categorisation of discrete or unordered data
 sets; for example, determining the level of risk (high/medium/low) associated
 with a certain financial investment given pre-existing market data.
 
\end_layout

\begin_layout Standard
Prediction is similar to classification, in that the process of learning
 from existing data is supervised, and a model is constructed.
 However, prediction techniques are used when assessing continuous data
 sets 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

; for example, determining the price of a stock market share at some point
 in the future given previous performance trends.
 Unlike prediction, clustering is not exclusively used for forcasting future
 values, rather grouping like points for immediate assessment.
\end_layout

\begin_layout Standard
Whether to utilise clustering in analysis is a decision based on the purpose
 of the data mining task.
 If future predictions are required, an alternative technique should be
 employed, but if like points in the set are to be grouped, clustering is
 an appropriate choice.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Comparison"

\end_inset

Comparison
\end_layout

\begin_layout Standard
This paper aims to draw a detailed comparison between two general clustering
 designations - density-based clustering and paritioning, and to do so,
 examines the differences between several specific techniques.
 By examining the differences in how each technique operates, the fundamental
 differences in their capabilities, and likewise to what circumstances are
 they most applicable, will be revealed.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:DBSCAN"

\end_inset

DBSCAN
\end_layout

\begin_layout Standard
The DBSCAN clustering technique, first proposed in 1996 by Ester et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

, requires two user-inputted parameters: 
\emph on
minPoints
\emph default
, defined as the minimum number of points required to exist in a neighbourhood
 to be declared a cluster, and 
\begin_inset Formula $\epsilon$
\end_inset

, defined as the radius of the neighbourhood of a point based on a distance
 metric.
 The minimum number of points is used to determine if a collection of points
 found should be considered a cluster, and the inputted 
\begin_inset Formula $\epsilon$
\end_inset

 value controls how far, whether it be in Euclidean, Manhattan or Minkowski
 distance, to consider for neighbours from a given data object.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:DBSCAN"

\end_inset

DBSCAN
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
For each unvisited point 
\begin_inset Formula $P$
\end_inset

 within the data set 
\begin_inset Formula $D$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Get the neighbours of the 
\begin_inset Formula $P$
\end_inset

, according to the given epsilon distance
\end_layout

\begin_layout Enumerate
If the number of neighbours is equal or greater than the user-specified
 cluster threshold
\end_layout

\begin_deeper
\begin_layout Enumerate
Increment cluster identifier counter
\end_layout

\begin_layout Enumerate
Add the given point to the current cluster
\end_layout

\begin_layout Enumerate
Recursively process all neighbours 
\end_layout

\end_deeper
\begin_layout Enumerate
Else if the point has fewer neighbours than the threshold value,
\end_layout

\begin_deeper
\begin_layout Enumerate
Mark 
\begin_inset Formula $P$
\end_inset

 as noise
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Standard
The process, as detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DBSCAN"

\end_inset

, begins by considering an arbitrary data object that has not already been
 assigned to a cluster.
 The neighbours of this data point 
\begin_inset Formula $P$
\end_inset

 are then located, according to the user-specified 
\begin_inset Formula $\epsilon$
\end_inset

 value.
 If the number of neighbours found is greater than the specified 
\emph on
minPoints
\emph default
 values, then the current data object should be added to the current cluster,
 and all neighbours should be processed recursively.
 Otherwise, if the number of near-by points is less than 
\emph on
minPoints
\emph default
, then the data object is considered noise.
 This process then continues for all of the neighbours, processing them
 accordingly.
 At such time that a given cluster has been fully explored, the process
 begins again, until all points have been visisted.
 At its conclusion, all defined clusters are finalised; no further points
 can be added to any cluster and any unclustered points are declared as
 
\emph on
noise
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscan.png
	lyxscale 50
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-stage-of"

\end_inset

Initial stage of DBSCAN technique
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
An initial processing stage for the DBSCAN procedure is shown in a simplifed
 data set within Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-stage-of"

\end_inset

, where the dashed line represents a two-dimensional 
\begin_inset Formula $\epsilon$
\end_inset

 distance radius and the minimum points threshold is 1.
\end_layout

\begin_layout Subsection
OPTICS
\end_layout

\begin_layout Standard
The Ordering Points to Identify the Clustering Structure (OPTICS) algorithm
 is structurally identitical to that of the previously mentioned DBSCAN
 
\begin_inset CommandInset citation
LatexCommand cite
key "ankerst1999oop"

\end_inset

.
 Thus its algorithm is similar to that shown in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DBSCAN"

\end_inset

, and likewise, its time complexity is the same also.
 
\end_layout

\begin_layout Standard
The OPTICS technique builds upon DBSCAN by attempting to overcome the issue
 of needing to input different parameters by introducing new values that
 need to be stored with each data object.
 Specifically, these are referred to as the 
\emph on
core-distance
\emph default
, the smallest epsilon value that makes a data object a core object, and
 the 
\emph on
reachability-distance, 
\emph default
which is a measure of distance between a given object and another.
 The 
\emph on
reachability-distance 
\emph default
is calculated as the greater of either the 
\emph on
core-distance
\emph default
 of the data object or the Euclidean distance between the data object and
 another 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 
\end_layout

\begin_layout Standard
These newly introduced distances are used to order the objects within a
 given set of data.
 Clusters become extracted based upon the reachability information and core
 distances associated with each object.
 From this, clusters are then able to be modelled according to reachability
 and cluster assignments, thus potentially revealing more relevant information
 about the attributes for each cluster.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:K-Medoids"

\end_inset

K-Medoids
\end_layout

\begin_layout Standard
The K-Medoids clustering technique is concerned with the concept of paritioning
 data objects.
 The technique was first developed in 1987 by Kaufman & Rousseeuw and seeks
 to reduce the impact of noisy data experienced in simpler partitioning
 algorithms, such as K-Means 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1987cmm"

\end_inset

.
 Instead of converting data attributes to simple points, it retains all
 the information from each record and represents the points as objects on
 a detailed axes.
 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:K-Medoids"

\end_inset

K-Medoids
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily select 
\begin_inset Formula $k$
\end_inset

 objects as initial medoid points from given data set
\end_layout

\begin_layout Enumerate
Whilst swaps are occuring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data object in the data set with its closest medoid
\end_layout

\begin_layout Enumerate
Randomly select a non-medoid object, 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:Compute-the-total"

\end_inset

Compute the total cost of swapping initial medoid object with 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
If the total cost of swapping is less than the total cost of the current
 system then
\end_layout

\begin_deeper
\begin_layout Enumerate
Swap the inital medoid with 
\begin_inset Formula $O$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Continue until there is no change
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Standard
This method, as detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Medoids"

\end_inset

, utilises an inputted value, 
\begin_inset Formula $k$
\end_inset

, to segment a given database of objects into 
\begin_inset Formula $k$
\end_inset

 clusters.
 Each cluster exists such that all contained data objects are most closely
 associated with the medoid of the cluster.
 The process begins with the initial selection of 
\emph on
k
\emph default
 arbitrary points as representative medoids, and then computes cluster assignmen
ts for each of the data objects.
 Thus, at this stage, each point will be thereby associated with a medoid
 from the 
\emph on
k
\emph default
 medoids available.
\end_layout

\begin_layout Standard
The algorithm then continues by selecting a random non-medoid data object,
 O
\begin_inset Formula $_{r}$
\end_inset

, and recalculating what the potential benefit, if any, would be if it were
 to swapped with its current medoid.
 If there is a benefit, that is to say that the cost of having the medoid
 swapped, then the system should make the change and continue.
 This process continues until the system in such a state that every data
 object has been assigned to a cluster, and that the swapping process no
 longer optimises the clustering outcome 
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

.
\end_layout

\begin_layout Standard
The decision to swap a medoid in this technique is based on a cost function;
 that is, the total distance between all points and their respective medoids.
 Similarly to DBSCAN, the term 
\emph on
distance
\emph default
 defined here may refer, depending on the implementation, to a Euclidean,
 Manhattan or Minkowski metric.
\end_layout

\begin_layout Subsection
K-Means
\end_layout

\begin_layout Standard
The K-Means technique is similar to that of K-Medoids in that both methods
 use the same designations of input variables, and both attempt to cluster
 data objects in related manners.
 Whilst K-Medoids is concerned with using data objects themselves as focus
 points for a cluster, K-Means, however, utilises the centroid, or 
\emph on
mean
\emph default
, of a cluster for its computations.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:K-Means"

\end_inset

K-Means
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily or heuristically select 
\begin_inset Formula $k$
\end_inset

 objects as initial centroids for the given data set
\end_layout

\begin_layout Enumerate
Whilst centroid changes are occuring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data point 
\begin_inset Formula $P$
\end_inset

 in the data set to its closest centroid
\end_layout

\begin_layout Enumerate
For each centroid 
\begin_inset Formula $C$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Recompute location according to data points associated with it
\end_layout

\end_deeper
\begin_layout Enumerate
Continue until stopping criteria met
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Standard
The method, as described in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Means"

\end_inset

, begins much the same way that K-Medoids does - by using some process to
 select 
\emph on
k
\emph default
-many objects as initial mean objects from the data set.
 From this, each data object is iteratively assigned to its respective cluster,
 according to its nearest centroid point.
 Once this has been completed, then the centroid for each cluster is recomputed
 accordingly.
 This process will continue until the data set has reached a state of convergenc
e.
 That is to say that some stopping condition has been reached, and that
 the clustering results have thereby been sufficiently optimised 
\begin_inset CommandInset citation
LatexCommand cite
key "Hartigan1979"

\end_inset

.
\end_layout

\begin_layout Subsection
Capability comparison
\end_layout

\begin_layout Standard
Each of the clustering algorithms mentioned previously 
\end_layout

\begin_layout Standard
What's the result of comparing these individual algorithms:
\end_layout

\begin_layout Itemize
Need to select input values -> Can arbitrary numbers of clusters be found
 or not?
\end_layout

\begin_layout Itemize
Different sizes and shapes of clusters - can handle them or not?
\end_layout

\begin_layout Itemize
Sensitivity to noise?
\end_layout

\begin_layout Itemize
Scalability, efficiency, time complexity, attribute dimensionality
\end_layout

\begin_layout Itemize
Draw conclusions about the types of data that each technique is most suited
 for
\end_layout

\begin_layout Standard
The DBSCAN algorithm, like other density-based techniques, is most suited
 to data sets where the presence of noise would otherwise affect results,
 and also when the number, shape, or size of clusters present are not known.
 These are two benefits which are not available from the partitoning techniques
 - K-Medoids or K-Means - under review.
 Both of these algorithms are sensitive to noise 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Customisations"

\end_inset

Customisations
\end_layout

\begin_layout Standard
As previously mentioned, the subject area under consideration has been investiga
ted and re-investigated many times over its lifetime.
 As such, many previous works have discussed ways in which clustering algorithms
 can be optimised and made otherwise more resilient.
 This paper attemps to add to already voluminous amount of work by introducing
 two different techniques to approaching the DBSCAN and K-Medoids algorithms.
 These two techniques were chosen for analysis as they both can operate
 successfully, in terms of the results produced, and are well established
 and known, in terms of past quantities of research.
\end_layout

\begin_layout Standard
It should be noted that these customisations aim only to modify the techniques
 specified, rather than to construct extended methodologies.
 The reasoning for this is that any significantly complex modification to
 an existing clustering technique would ultimately affect its speed and
 performance, whilst leaving core functionality the same.
 As this paper focuses upon performance as a key aspect of a clustering
 algorithm, it would take research beyond the scope of this work to investigate
 further performance enhancements.
 Thus, whilst this paper does not suggest entirely new clustering techniques,
 the modifications suggested do appear to have their respective uses.
\end_layout

\begin_layout Subsection
DBSCAN
\end_layout

\begin_layout Standard
Some authors 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 highlight that one issue with DBSCAN, and similarly, many other clustering
 techniques, is the necessity to correctly select input parameters.
 In the case of DBSCAN, these values are the minimum number of points needed
 to form a cluster, 
\emph on
minPoints,
\emph default
 and the distance value, 
\begin_inset Formula $\epsilon$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:DBSCAN"

\end_inset

 .
 As can be expected, if a set of input variables are incorrectly selected
 - whereby the margin between correct and incorrect may vary greatly, depending
 on the data set - then the results may be completely unexpected, and thus
 rendered useless.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
asfdas
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Customised-DBSCAN-procedure"

\end_inset

Customised DBSCAN procedure
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This paper suggests that the DBSCAN algorithm offer the ability to dynamically
 modify the 
\begin_inset Formula $\epsilon$
\end_inset

 distance value as necessary, rather than relying upon a static, potentially
 unsuitable, distance across all data objects within a collection.
 To achieve this change, the distance value is adjusted according to how
 many points are present within the current cluster.
 This modification aims to result in a decrease in the distance checked
 surrounding each subsequent point within a cluster, in an attempt to reduce
 the potential for data instances being assigned to a cluster when they
 should otherwise not belong.
 Consider a dense cluster of objects, such as that in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Customised-DBSCAN-procedure"

\end_inset

, and that this cluster also has a number of other noise objects surrounding
 it.
 Using a constant distance value in this situation would be ineffective
 and would result in the objects in the figure that are marked as noise
 being otherwise assigned to the cluster incorrectly.
 The consideration in these situations is that a very dense cluster will
 see its members similarly spaced.
 For any given 
\begin_inset Formula $\epsilon$
\end_inset

, if many data objects are found using such a value, then 
\begin_inset Formula $\epsilon$
\end_inset

 can be decreased to reduce the effect of noise, and also lessen the potential
 for the 
\emph on
bridging effect
\emph default
, whereby two clusters are inadvertedly joined.
\end_layout

\begin_layout Subsection
K-Medoids
\end_layout

\begin_layout Standard
As previously mentioned in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:K-Medoids"

\end_inset

, one of the biggest issues that exists with the K-Medoids algorithm is
 the computational time required to determine the suitability of a given
 random object becoming a new medoid 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 This overhead is larely unnecessary, as a system will frequently be in
 such a state that the reassignment of a given medoid will not affect a
 large amount of data objects present.
 This would especially be the case for a large data set that consists of
 many clusters; a single medoid change would only affect the small number
 of objects and clusters nearby to it.
 
\end_layout

\begin_layout Standard
Within the original algorithm, detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:K-Medoids"

\end_inset

, the step described at 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:Compute-the-total"

\end_inset

 is the one which has been determined to be the most superfluous.
 At present, calculating the cost of swapping one medoid for another is
 significant, with the time complexity of this one sub-operation resulting
 as being 
\begin_inset Formula $O(n)$
\end_inset

, where 
\begin_inset Formula $n$
\end_inset

 is the number of objects present within the given database.
 When one considers this operation in context, that the entire set of data
 objects need to be iterated over each time a swap, or potential swap, occur,
 the overhead is evident.
\end_layout

\begin_layout Standard
A suitable customisation to the K-Medoids procedure is to attempt to limit
 the number of times the data objects each need to be processed, per iteration
 of swaps of medoids.
 This suggested concept requires the clustering associations, and thereby
 the distances from a given data object to its related medoid, be recalculated
 at most only once when a medoid object is changed.
 At present, the algorithm recomputes the cluster and medoid assignments
 for all points in two instances: intially or after a swap has occurred,
 and also in order to determine the total cost of swapping a medoid within
 the system.
 In many situations, this would be grossly inefficient as most points should
 and will remain associated with the same medoid.
 The only data instances that would change are those which have close to
 dual-medoid membership, given the location of the new, potential medoid.
 With the suggested customiations, the algorithm only recalculates cluster
 membership after a change has been made, and rather than recomputing all
 memberships at the second instance, it instead swaps a given medoid out
 temporarily with the other randomly selected medoid.
\end_layout

\begin_layout Standard
Because the second iteration through the entire data set is made redundant,
 the total cost of swapping medoids can be made at the same time that clustering
 assignments are made.
 The result of this change is a significant improvement in time complexity,
 reducing the original system from 
\begin_inset Formula $O(2n)$
\end_inset

 to 
\begin_inset Formula $O(n)$
\end_inset

, whereby 
\begin_inset Formula $n$
\end_inset

 is the number of objects within the given set of data.
 Put into practice, the results of experiements performed on this specialised
 algorithm are show in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Results-and-Discussion"

\end_inset

Results and Discussion
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Details"

\end_inset

Experimental Details
\end_layout

\begin_layout Standard
The experimentation detailed in this section was carried out within the
 Waikato Environment for Knowledge Analysis (WEKA) suite for machine learning.
 This software, developed in the Java programming language, offers a powerful
 testing harness for analysis of various data mining concepts and implementation
s.
 In addition, it offers the ability to extend the suite with additional
 modules, and in the case of the presented work, clustering methods.
 
\end_layout

\begin_layout Standard
Building upon the functionality that already exists within WEKA, several
 existing techniques - specifically, DBSCAN and K-Medoids - were implemented,
 as were the customisations, as detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Customisations"

\end_inset

.
 Once developed, these clustering algorithms were applied the specified
 test data set of forest fire data to determine their effectiveness, in
 terms of the number of clusters found and noise points classified, and
 efficiency, in terms of running time complexity.
 For each different input and clustering technique, the effects of the previousl
y-mentioned customisations were also noted, and corresponding results recorded.
 Analysis of these results aimed to demonstrate if these modifications were
 able to offer any degree of improvement in the results produced over their
 original algorithmic counterparts.
\end_layout

\begin_layout Standard
The tests were performed on the same computing hardware in order to ensure
 objectivity - a recent-model desktop computer with a 2.5GHz Intel Centrino
 dual-core processor, 3GB of RAM, and running the latest stable release
 of the Java Virtual Machine (JVM), version 1.6.
 These experiments examined a range of varying input parameters to determine
 their effect on an algorithm's results, and to determine any changes in
 speed.
 
\end_layout

\begin_layout Subsection
Comparison Results
\end_layout

\begin_layout Subparagraph*
Density Based
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeed.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-Performance-Results"

\end_inset

Density-based performance by epsilon
\end_layout

\end_inset


\end_layout

\end_inset

Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-Performance-Results"

\end_inset

 shows a detailed comparison of performance of various density-based clustering
 algorithms, and demonstrates the speed of clustering, based upon a differing
 epsilon distance value.
 The epsilon value is varied to determine its effect upon each technique,
 given that it could be expected that a larger epsilon value would see generally
 larger clusters with greater quantities of members in each cluster.
 
\end_layout

\begin_layout Standard
As detailed in the figure, each of these algorithms operates in a similar
 time complexity.
 What is evident, however, is that generally, as epsilon increases, there
 will be a marginal increase in the amount of time necessary to complete
 the clustering operation.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement p
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeedattrib.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-attrib"

\end_inset

Density-based performance by attributes
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscannoise.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-noise"

\end_inset

Density-based performance by resulting noise
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoidsspeed.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Partitioning-performance-k"

\end_inset

Partitioning performance by 
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoidspeedattrib.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Partitioning-performance-attrib"

\end_inset

Partitioning performance by attributes
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Conclusions
\end_layout

\begin_layout Standard
As we detail the results, we should offer a discussion as to what they are,
 and why they are like that.
\end_layout

\begin_layout Standard
Eg draw conclusions here and offer explanations
\end_layout

\begin_layout Section
Issues
\end_layout

\begin_layout Standard
As is to be expected of any data mining procedure, the best results from
 a clustering operation depend entirely upon many different factors.
 These include, but are not limited to the actual data set being processed,
 the clustering technique selected, and the input parameters provided to
 such technique.
 In some situations, a certain algorithm may determine suitable clusters
 within one data set, but may be completely ineffective with another.
 Likewise, using unsuitable input parameters for a given clustering process
 may result in similarly unsuitable cluster assignments for data instances.
 All of these aspects of data mining thereby need to be reviewed before
 being used, and perferably by a person involved who has sufficient domain
 knowledge to make correct descisions that will reveal suitable output.
\end_layout

\begin_layout Standard
Furthermore, as detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

, the most suitable types of data are detailed for the given subsets of
 clustering techniques.
 It should be noted that the results demonstrated in this paper describe
 tests executed within the given testing environment, and only represent
 results associated with the one data set of forest fire information being
 used.
 Refer to Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Data-Set"

\end_inset

 for details regarding this test data set.
 As mentioned previously, using a different set of data may, and most likely
 will, reveal different patterns and see a change in processing time complexity.
 That said, the results detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

 indicate for the data set under consideration, the DBSCAN and K-Medoids
 algorithms, and their customised counterparts, reveal useful clusters.
 From this, it can be generally inferred that for similar types of data,
 these same algorithms should result in similar types of clusters being
 found and similar durations of processing required.
\end_layout

\begin_layout Standard
In summary, 
\end_layout

\begin_layout Standard
Discuss the results in this section
\end_layout

\begin_layout Standard
Is there sufficient evidence here to show that our algorithms are worthy
 of use?
\end_layout

\begin_layout Standard
Issues - What drawbacks do we see with our various sets of data and algorithms
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Conclusion"

\end_inset

Conclusion
\end_layout

\begin_layout Standard
Overall: better or worse? Able to satisfy or not-possible? It might not
 be given the complexity and time limitations
\end_layout

\begin_layout Standard
summarise topic and close.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Future-Work"

\end_inset

Future Work
\end_layout

\begin_layout Standard
Describe potential improvements to the methodologies we've presented within
 this paper.
 Others can extend our work by seeking other ways of improving the semantics
 of the algorithm, rather than attempting to improve its operation itself.
 Given that these algorithms this paper discusses have been in existence
 for <X> years, and that there have been many various papers and theses
 produced regarding these issues, this appears the logical path for improvements
 to take.
 Should any future developers decide to follow through on the same path,
 this paper has logically set out its concepts and ideas, and will provide
 a solid basis for such research.
\end_layout

\begin_layout Standard
As the topic of data mining, and more specifically, clustering, becomes
 more of a part of every-day business and organisational operation, the
 necessity for faster and equally accurate algorithms rises.
 This paper has shown this, and several other applications do exist, and
 it can only be assumed that 
\end_layout

\begin_layout Standard
There are many other potential improvements that could be investigated with
 clustering techniques - extensions to the work presented here are essentially
 unable to be bounded.
 Suggestions that come to mind would be to attempt to limit the importance
 of input variables and their effect on the results of clustering.
 As methodologies stand at present, incorrect parameters can result in vastly
 different clustering results being generated.
 By being able to reduce or eliminate the presence and effects these variables
 have on results, a system will be able to produce more objective results
 with greater accuracy.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "clustersdk"
options "latex8"

\end_inset


\end_layout

\end_body
\end_document
