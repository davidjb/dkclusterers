#LyX 1.6.0rc3 created this file. For more info see http://www.lyx.org/
\lyxformat 340
\begin_document
\begin_header
\textclass latex8
\begin_preamble

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%


\usepackage{latex8}

\usepackage{times}

\usepackage{epsf}

\usepackage{epsfig}

\usepackage[config, font={sf,bf}]{caption}

\usepackage[config, font={small, sf}]{subfig}

\usepackage{latexsym}
   % additional packages that may be read in
    % to augment generic LaTeX; needed for \mathbb

\newcommand{\VD}{{Voronoi diagram}}
\newcommand{\VDs}{{Voronoi diagrams}}
%\documentstyle[times,art10,twocolumn,latex8]{article}

%-------------------------------------------------------------------------
% take the % away on next line to produce the final camera-ready version


%-------------------------------------------------------------------------
\end_preamble
\options times
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle empty
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Clusterers: a Comparison of Partitioning and Density-Based Algorithms and
 a Discussion of Optimisations
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
author{David Breitkreutz
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{
\backslash
& Information Technology} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia
\backslash

\backslash
 David.Breitkreutz@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
and
\end_layout

\begin_layout Plain Layout

Kate Casey
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{School of Mathematics, Physics} 
\backslash

\backslash
 
\end_layout

\begin_layout Plain Layout


\backslash
textit {
\backslash
& Information Technology} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
textit{James Cook University} 
\backslash

\backslash
 Townsville, QLD 4811, Australia 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Kate.Casey@jcu.edu.au
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Though data mining is a relatively recent innovation, the improvements it
 offers over traditional data analysis have seen the field expand rapidly.
 Given the critical requirement for the efficient and accurate delivery
 of useful information in today's data-rich climate, significant research
 in the topic continues.
\end_layout

\begin_layout Abstract
Clustering is one of the fundamental techniques adopted by data mining tools
 across a range of applications.
 It provides several algorithms that can assess large data sets based on
 specific parameters and group related data points.
\end_layout

\begin_layout Abstract
This paper compares two widely used clustering algorithms, K-Medoids and
 Density-Based Spatial Clustering of Applications with Noise (DBSCAN), against
 other well-known techniques.
 The initial testing conducted on each technique utilises the standard implement
ation of each algorithm.
 Further experimental work tests modifications to these methods in order
 to investigate potential improvements of results or efficiency.
 Various key applications of clustering methods are detailed, and several
 areas of future work have been suggested.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Introduction"

\end_inset

Introduction 
\end_layout

\begin_layout Standard
Detail why clustering is powerful, why are we using it, what applications
 it applies to the most
\end_layout

\begin_layout Standard
Detail brief outline of clustering techniques (partitioning, density etc)
 and what they are most suited for (respectively)
\end_layout

\begin_layout Standard
Highlight what this paper's purpose is (to compare and contrast dbscan and
 k-medoids), and how it aims to achieve this (experimentation using WEKA
 to compare efficiency and accuracy of results)
\end_layout

\begin_layout Standard
Highlight that this paper will also attempt to suggest improvements to these
 algorithms (in order to try and obtain better results), and demonstrates
 its findings in this area
\end_layout

\begin_layout Standard
Lead into the related work section (eg many papers that already exist, and
 we talk about them now in Section 2)
\end_layout

\begin_layout Subsection
Importance
\end_layout

\begin_layout Standard
Clustering is promoted as an extremely powerful means of grouping related
 data points, and can efficiently reveal highly relevant trends in a source
 data set.
 These capabilities extend to large sets of data, and are applicable to
 stores of information used by scientists, researchers, and businesses.
 As a result, the field has developed into one of the foremost research
 areas in modern computing.
 These data mining techniques can analyse both large-scale databases and
 data warehouses.
 There exist numerous algorithms for performing such analysis, and each
 may be more suitable to certain circumstances, depending entirely upon
 domain-specific parameters.
 As such, the efficiency and accuracy of results from these data mining
 tasks relies directly upon the choice of a suitable algorithms.
 Thus, for as many different types of data set that exist, there is the
 requirement for further research into the improvements of these techniques.
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Data mining in general has a multitude of applications across a wide variety
 of fields.
 Pattern recognition for image analysis, medical diagnostics, sales forcasting
 and weather prediction are a recognised as a few of the more traditional
 usages.
 However, due to extensive development in the field and the recent explosion
 in data recording the capabilities extend far beyond these basic functions.
 Onboard computer analysis in vehicles, product quality analysis, targeted
 advertising campaigns, spam email filtration, fraud detection, and online
 crime analysis are but a few of the fields into which data mining now extends.
 
\begin_inset CommandInset citation
LatexCommand cite
key "john1999bsd"

\end_inset

 Clustering applies to all of the aforementioned applications as a subset
 of data mining.
 
\end_layout

\begin_layout Standard
A prominent example of this functionality is the application of emergency
 situation data analysis; for example, data recorded by authorities during
 forest fires.
 This paper will investigate that specific scenario in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Details"

\end_inset

, with the test sets utilised containing fire data from Portugal
\emph on
.
\end_layout

\begin_layout Subsection
Other Methods
\end_layout

\begin_layout Standard
There are a wide variety of clustering methodologies that exist within this
 field.
 These include, but are not limited to, grid-based categorisation, density-based
 grouping, hierarchical segmentation, and constraint-based analysis.
 As these techniques have been already widely researched, there are many
 existing works that compare and constrast each.
 The interested reader is directed to 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 for further information regarding these topics.
\end_layout

\begin_layout Standard
The research outlined in this paper is concerned with the application of
 two of the most widely used methods; 
\emph on
partitioning-based 
\emph default
and 
\emph on
density-based
\emph default
 clustering.
\end_layout

\begin_layout Standard
Partitioning algorithms are effective for mining data sets when computation
 of a dendrogram (clustering tree) representation is infeasible.
 
\begin_inset CommandInset citation
LatexCommand cite
key "jain1999dcr"

\end_inset

 In particular, K-Medoids is highly efficient for moderately sized data
 sets with spherical-type clusters.
 However, due to the inherent swapping of medoids to optimise the clustered
 solution, the algorithm suffers greatly as large data sets are introduced.
 
\end_layout

\begin_layout Standard
Density-based algorithms perform optimally when operating upon spatially-indexed
 data.
 The methods provide benefits when analysing data sets that contain high
 levels of noise or when clusters are arbitrarily shaped.
 Specifically, DBSCAN is able to grow clusters whilst the MinPts threshold
 is not satisfied within the a specified neighbourhood, thus efficiently
 dividing real data and noise in a variety of shapes.
\end_layout

\begin_layout Subsection
Purpose of Paper
\end_layout

\begin_layout Standard
This paper focuses upon the DBSCAN and K-Medoids algorithms from the density-bas
ed and partitioning families respectively.
 It outlines the design, advantages and disadvantages of each method, and
 studies similar methodologies.
 The work contained goes on to propose both basic implementations of the
 techniques, as well as initiating new research into improving the efficiency
 of the methods.
 A great deal of literature already exists regarding these techniques, and
 a review of these documents can be found in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Data-Set"

\end_inset

Experimental Data Set
\end_layout

\begin_layout Standard
The data set chosen for the comparison of and ammendments to the K-Medoids
 and DBSCAN techniques is 
\emph on
forest fires
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "cortez:dma"

\end_inset

.
 The set contains environmental data from fire instances in northeastern
 Portugal, and represents instances within space, and includes attributes
 such as the date, temperature, humidity, wind conditions, rain information
 and burned area.
 
\end_layout

\begin_layout Standard
Analysing this data set is necessary because forest fires cause millions
 of dollars damage to property and claim many lives each year.
 If clustering techniques can be used to more accurately determine the patterns
 of such fires given provailing environmental conditions, scientists and
 researchers will be able to achieve further understand of the phenomenon.
 This knowledge will reap enormous benefits; predictive techniques can be
 updated, analysis centres local to fires can more accurately forcast fire
 behaviour, government agencies can be informed of these conclusions earlier,
 and, ultimately, residents and business owners can be warned earlier.
 [SAVE MONEY AND LIVES]
\end_layout

\begin_layout Standard

\emph on
[Describe what the specific data set we've chosen here, and why our selected
 algorithm(s) are worthwhile to be 
\emph default
executed
\emph on
 here.
 Why is the data set relevant to some aspect of life on earth (and why should
 we care)? - this makes the problem important to tackle, and should make
 people interested]
\end_layout

\begin_layout Subsection
Results Overview
\end_layout

\begin_layout Standard

\emph on
[Brief introduction as to the results of the comparison were..
 what the results indicate (overview) The improvements this paper suggests
 to both the DBSCAN and K-medoids algorithms show a level of improvement
 for the data set that is utilised for testing.
 This suggests potential for further improvement given additional levels
 of interest....]
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Related-Work"

\end_inset

Related Work
\end_layout

\begin_layout Standard
Detail the specific areas that we are looking at (density and partitioning),
 and what each is most useful for (shape of data, noise, features etc)
\end_layout

\begin_layout Standard
Provide a paragraph of two for hierarchical etc techniques that the we don't
 cover, and direct the user elsewhere
\end_layout

\begin_layout Standard
Detail a section for alternative data analysis methods to clustering (and
 why they might be useful elsewhere, but not here)
\end_layout

\begin_layout Subsection
Data Analysis
\end_layout

\begin_layout Standard
The process of data analysis involves considering all elements within a
 given source data set in order to deduce interesting patterns and trends.
 
\end_layout

\begin_layout Standard
What is data analysis first (used to obtain patterns and trends from a source
 set of data, with the information used to improve the decision making process
 (people can be more informed)
\end_layout

\begin_layout Subsection
Clustering
\end_layout

\begin_layout Standard
Detail what is clustering (own heading for this)
\end_layout

\begin_layout Standard
Aggarwal et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "aggarwal1999fap"

\end_inset

 define the process of clustering as:
\emph on
 
\begin_inset Quotes eld
\end_inset

Given a set of points in multidimensional space, find a partition of the
 points into clusters so that the points within each cluster are close to
 one another
\begin_inset Quotes erd
\end_inset


\emph default
.
 Proximity is measured using a variety of algorithm-specific metrics, such
 that the closer two arbitrary points are to one another, the more strongly
 they are considered to be related.
 This process results in defined groupings of similar points, where strong
 inter-cluster and weak intra-cluster relationships exist between points,
 and example of which is detailed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-clustered"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename clustering.png
	lyxscale 50
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-clustered"

\end_inset

Example of clustered points
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clustering can be categorised in machine learning terms as a form of 
\emph on
unsupervised learning
\emph default
; that is, clusters are representative of hidden patterns in the source
 data set
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

.
 Raw information is analysed and relationships are discovered by the algorithm
 without external direction or interference; learning through observation
 rather than by the study of examples
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 In addition, this objectivity translates to an effective means of data
 analysis without the opportunity for subjective human conclusions to be
 drawn from data.
\end_layout

\begin_layout Subsection
Partitioning Clustering Algorithms
\end_layout

\begin_layout Standard
Partitioning methods define clusters by grouping data points into 
\emph on
k
\emph default
 partitions (defined at runtime).
 A point is determined to be 
\emph on
similar
\emph default
 to other points within its partition, and 
\emph on
dissimilar
\emph default
 to points that lie outside the boundary of that partition.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 Comparison is based on the characteristics of the data set provided.
 Thus, the algorithms rely on the conversion of semantic data attributes
 (width, height, shape, colour, cost etc.) into points that determine physical
 location on a set of mathematical axes.
 This provides an objective and computationally acceptable framework for
 analysis.
 In the simplest case only two attributes exist, and thus the conversion
 renders a point on a standard Cartesian plane.
 This process is greatly complicated when, as often occurs in highly detailed
 source sets, hundreds of attributes are present.
 The rendering plane takes on high dimensionality, and the complexity of
 analysis becomes very computationally expensive.
 
\end_layout

\begin_layout Standard
This paper studies the K-Medoids algorithm from the partitioning analysis
 family.
 The technique was first developed in 1987 by Kaufman & Rousseeuw and seeks
 to reduce the impact of noisy data experienced in more simple partitioning
 algorithms.
 
\begin_inset CommandInset citation
LatexCommand cite
key "kaufman1987cmm"

\end_inset

 Instead of converting data attributes to simple points, it retains all
 the information from each record and represents the points as objects on
 a detailed axes.
 It initially chooses 
\emph on
k
\emph default
 arbitrary points as representative medoids (
\emph on
O
\begin_inset Formula $\mathcal{_{\text{r}}}$
\end_inset


\emph default
), then iteratively compares each chosen point with every other point (O
\begin_inset Formula $_{\text{j}}$
\end_inset

) to determine if swapping the medoid is useful.
 This decision is based on a cost function; the average value of some dissimilar
ity metric between any object and the medoid of its cluster.
 
\begin_inset CommandInset citation
LatexCommand cite
key "berkhin2002scd"

\end_inset

 The algorithm terminates when every point has been assigned to a cluster
 and swapping medoids and objects no longer has any optimising effect on
 the clustering outcome.
\end_layout

\begin_layout Standard
Partitioning offers several features, including efficiency in processing
 spherical clusters in small to medium data sets, and scalability across
 smaller applications.
 However, the algorithms suffer from extreme complexity and computational
 cost at higher dimensions in large sets, require the user to specify the
 number of clusters to be determined (
\emph on
k
\emph default
) and experience problems defining arbitrarily shaped clusters.
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Subsection
Density-Based Clustering Algorithms
\end_layout

\begin_layout Standard
The density-based group of clustering algorithms represent a data set in
 the same manner as partitioning methods; converting an instance to a point
 using the data attributes of the source set.
 The plane contains clusters with high internal density and low external
 density in a similar manner to its partitioning ancestor.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Kolatch01clusteringalgorithms"

\end_inset

 As a result, analysis can easily isolate noise instances from relevant
 instances.
 
\end_layout

\begin_layout Standard
This paper studies the DBSCAN algorithm from the density-based algorithm
 collection, first proposed in 1996 by Ester et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ester1996dba"

\end_inset

 The technique relies upon an epsilon value (
\begin_inset Formula $\mathcal{\varepsilon}$
\end_inset

 - defined as the radius of the neighbourhood of a point based on some distance
 metric), and a minimum points value (
\emph on
MinPts
\emph default
 - defined as the minimum number of points required to exist in a neighbourhood
 to be declared a cluster).
 The method examines the 
\begin_inset Formula $\varepsilon$
\end_inset

-neighbourhood of every point in the set, and if the region contains more
 points than the MinPts threshold a new cluster is defined.
 The examined point is declared the centre point, and the algorithm then
 iteratively collects surrounding density-reachable objects and attaches
 them to this new cluster.
 Such behaviour may trigger cluster merging depending upon point proximity
 and, of course, the runtime parameters 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $\varepsilon$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 and MinPts.
 The algorithm terminates when all defined clusters are finalised; no further
 points can be added to any cluster and any unclustered points are declared
 as 
\emph on
noise
\emph default
.
 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

 Due to this method of analysis, DBSCAN is designed to analyse data sets
 with arbitrary shapes, and can cluster sets that include hollow formations.
 
\end_layout

\begin_layout Standard
Density-based algorithms provide advantages over other methods through their
 noise handling capabilities and ability to determine clusters with arbitrary
 shapes (eg.
 a circle, torus or other non-convex formation).
 As with partitioning techniques, computational cost is a disadvantage when
 the technique is used with large amounts of source data and sets containing
 excessive noise.
 
\end_layout

\begin_layout Standard
[DIAGRAM HERE]
\end_layout

\begin_layout Subsection
Other Techniques
\end_layout

\begin_layout Standard
Clustering differs from other classical data mining methodologies in a variety
 of ways and extensive listerature already exists explaining this.
 For the purpose of conciseness, this paper will compare and contrast clustering
 only with the techniques of 
\emph on
classification
\emph default
 and 
\emph on
prediction
\emph default
.
\end_layout

\begin_layout Standard
Classification is the process of constructing a learning model, or 
\emph on
classifier
\emph default
, that assigns points in a dataset to predefined classes
\begin_inset CommandInset citation
LatexCommand cite
key "fayyad1996dma"

\end_inset

.
 Whilst this definition initially appears similar to that of clustering
 in that points are grouped based on attributes, any similarities are thus
 exhausted.
 The technique utilises 
\emph on
supervised learning 
\emph default
as opposed to the unsupervised learning environment of clustering.
 The model is compiled by studying provided data (known as a training set)
 containing class labels; hence the technique is 
\begin_inset Quotes eld
\end_inset

supervised
\begin_inset Quotes erd
\end_inset

 due to example data exposure.
 Once the model is complete it can be applied to real, unlabelled data and
 assign each new tuple to its respective class
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 Classification is used in the categorisation of discrete or unordered data
 sets; for example, determining the level of risk (high/medium/low) associated
 with a certain financial investment given pre-existing market data.
 
\end_layout

\begin_layout Standard
Prediction is similar to classification, in that the process of learning
 from existing data is supervised, and a model is constructed.
 However, prediction techniques are used when assessing continuous data
 sets 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

; for example, determining the price of a stock market share at some point
 in the future given previous performance trends.
 Unlike prediction, clustering is not exclusively used for forcasting future
 values, rather grouping like points for immediate assessment.
\end_layout

\begin_layout Standard
Whether to utilise clustering in analysis is a decision based on the purpose
 of the data mining task.
 If future predictions are required, an alternative technique should be
 employed, but if like points in the set are to be grouped, clustering is
 an appropriate choice.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Comparison"

\end_inset

Comparison
\end_layout

\begin_layout Standard
This paper aims to draw a detailed comparison between two general clustering
 designations - density-based clustering and paritioning, and to do so,
 examines the differences between several specific techniques.
\end_layout

\begin_layout Standard
Comparing the specific techniques themselves (so we've detailed what the
 types of clustering are in section 2, here we talk about what the technique
 of DBSCAN etc does
\end_layout

\begin_layout Standard
big O stuff??.
\end_layout

\begin_layout Standard
will write algorithms out
\end_layout

\begin_layout Subsection
DBSCAN
\end_layout

\begin_layout Standard
The DBSCAN clustering technique requires two user-inputted parameters: a
 minimum number of points, 
\emph on
minPoints
\emph default
, and an epsilon distance, 
\begin_inset Formula $\epsilon$
\end_inset

.
 The minimum number of points is used to determine if a collection of points
 found should be considered a cluster, and the inputted 
\begin_inset Formula $\epsilon$
\end_inset

 value controls how far, whether it be in Euclidean, Manhattan or Minkowski
 distance, to consider for neighbours from a given data object.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:DBSCAN"

\end_inset

DBSCAN
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
For each unvisited point 
\begin_inset Formula $P$
\end_inset

 within the data set 
\begin_inset Formula $D$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Get the neighbours of the 
\begin_inset Formula $P$
\end_inset

, according to the given epsilon distance
\end_layout

\begin_layout Enumerate
If the number of neighbours is equal or greater than the user-specified
 cluster threshold
\end_layout

\begin_deeper
\begin_layout Enumerate
Increment cluster identifier counter
\end_layout

\begin_layout Enumerate
Add the given point to the current cluster
\end_layout

\begin_layout Enumerate
Recursively process all neighbours 
\end_layout

\end_deeper
\begin_layout Enumerate
Else if the point has fewer neighbours than the threshold value,
\end_layout

\begin_deeper
\begin_layout Enumerate
Mark 
\begin_inset Formula $P$
\end_inset

 as noise
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Standard
The process, as detailed in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DBSCAN"

\end_inset

, begins by considering an arbitrary data object that has not already been
 assigned to a cluster.
 The neighbours of this data point 
\begin_inset Formula $P$
\end_inset

 are then located, according to the user-specified 
\begin_inset Formula $\epsilon$
\end_inset

 value.
 If the number of neighbours found is greater than the specified 
\emph on
minPoints
\emph default
 values, then the current data object should be added to the current cluster,
 and all neighbours should be processed recursively.
 Otherwise, if the number of near-by points is less than 
\emph on
minPoints
\emph default
, then the data object is considered noise.
 This process then continues for all of the neighbours, processing them
 accordingly.
 At such time that a given cluster has been fully explored, the process
 begins again, until all points have been visisted.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscan.png
	lyxscale 50
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial-stage-of"

\end_inset

Initial stage of DBSCAN technique
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
An initial processing stage for the DBSCAN procedure is shown in a simplifed
 data set within Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial-stage-of"

\end_inset

, where the dashed line represents a two-dimensional 
\begin_inset Formula $\epsilon$
\end_inset

 distance radius and the minimum points threshold is 1.
\end_layout

\begin_layout Subsection
OPTICS
\end_layout

\begin_layout Standard
The Ordering Points to Identify the Clustering Structure (OPTICS) algorithm
 is structurally identitical to that of the previously mentioned DBSCAN
 
\begin_inset CommandInset citation
LatexCommand cite
key "ankerst1999oop"

\end_inset

.
 Thus its algorithm is similar to that shown in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DBSCAN"

\end_inset

, and likewise, its time complexity is the same also.
 
\end_layout

\begin_layout Standard
The OPTICS technique builds upon DBSCAN by attempting to overcome the issue
 of needing to input different parameters by introducing new values that
 need to be stored with each data object.
 Specifically, these are referred to as the 
\emph on
core-distance
\emph default
, the smallest epsilon value that makes a data object a core object, and
 the 
\emph on
reachability-distance, 
\emph default
which is a measure of distance between a given object and another.
 The 
\emph on
reachability-distance 
\emph default
is calculated as the greater of either the 
\emph on
core-distance
\emph default
 of the data object or the Euclidean distance between the data object and
 another 
\begin_inset CommandInset citation
LatexCommand cite
key "han2001dmc"

\end_inset

.
 
\end_layout

\begin_layout Standard
These newly introduced distances are used to order the objects within a
 given set of data.
 Clusters become extracted based upon the reachability information and core
 distances associated with each object.
 From this, clusters are then able to be modelled according to reachability
 and cluster assignments, thus potentially revealing more relevant information
 about the attributes for each cluster.
\end_layout

\begin_layout Subsection
K-Medoids
\end_layout

\begin_layout Standard
The K-Medoids clustering technique utilises an inputted value, 
\begin_inset Formula $k$
\end_inset

, to segment a given database of objects into 
\begin_inset Formula $k$
\end_inset

 clusters.
 Each cluster exists such that all contained data objects are most closely
 associated with the medoid of the cluster.
\end_layout

\begin_layout Standard
The minimum number of points is used to determine if a collection of points
 found should be considered a cluster, and the inputted 
\begin_inset Formula $\epsilon$
\end_inset

 value controls how far, whether it be in Euclidean, Manhattan or Minkowski
 distance, to consider for neighbours from a given data object.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:K-Medoids"

\end_inset

K-Medoids
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily select 
\begin_inset Formula $k$
\end_inset

 objects as initial medoid points from given data set
\end_layout

\begin_layout Enumerate
Whilst swaps are occuring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data object in the data set with its closest medoid
\end_layout

\begin_layout Enumerate
Randomly select a non-medoid object, 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
Compute the total cost of swapping initial medoid object with 
\begin_inset Formula $O$
\end_inset


\end_layout

\begin_layout Enumerate
If the total cost of swapping is less than the total cost of the current
 system then
\end_layout

\begin_deeper
\begin_layout Enumerate
Swap the inital medoid with 
\begin_inset Formula $O$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Continue until there is no change
\end_layout

\end_deeper
\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
K-Means
\end_layout

\begin_layout Standard
The K-Means technique is 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:K-Means"

\end_inset

K-Means
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Arbitrarily or heuristically select 
\begin_inset Formula $k$
\end_inset

 objects as initial centroids for the given data set
\end_layout

\begin_layout Enumerate
Whilst centroid changes are occuring 
\end_layout

\begin_deeper
\begin_layout Enumerate
Associate each data point 
\begin_inset Formula $P$
\end_inset

 in the data set to its closest centroid
\end_layout

\begin_layout Enumerate
For each centroid 
\begin_inset Formula $C$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Recompute location according to data points associated with it
\end_layout

\end_deeper
\begin_layout Enumerate
Continue until stopping criteria met
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Subsection
Comparison
\end_layout

\begin_layout Standard
What's the result of comparing these individual algorithms:
\end_layout

\begin_layout Itemize
Need to select input values -> Can arbitrary numbers of clusters be found
 or not?
\end_layout

\begin_layout Itemize
Different sizes and shapes of clusters - can handle them or not?
\end_layout

\begin_layout Itemize
Sensitivity to noise?
\end_layout

\begin_layout Itemize
Scalability, efficiency, time complexity, attribute dimensionality
\end_layout

\begin_layout Itemize
Draw conclusions about the types of data that each technique is most suited
 for
\end_layout

\begin_layout Standard
The DBSCAN algorithm, like other density-based techniques, is most suited
 to data sets where the presence of noise would otherwise affect results,
 and also when the number, shape, or size of clusters present are not known.
 These are two benefits which are not available from the partitoning techniques
 - K-Medoids or K-Means - under review.
 Both of these algorithms are sensitive to noise 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Customisations"

\end_inset

Customisations
\end_layout

\begin_layout Standard
Detail here what we did to modify the algorithms that we implemented
\end_layout

\begin_layout Standard
Why were DBSCAN and K-Medoids 
\end_layout

\begin_layout Subsection
UltraDBSCAN
\end_layout

\begin_layout Standard
Dynamically modify the value of epsilon such that it becomes adjusted according
 to how many points are present within the current cluster.
 This modification was considered for use within data sets that a constant
 distance value would be ineffective.
 By reducing the epsilon value according to current cluster size, the clusters
 found aim to be denser, and the potential for the 'bridging effect' is
 attempted to be minimised.
 The consideration here is that a very dense cluster will see its members
 similarly spaced, and for any given epislon value, if many clusters are
 found under such a value, then epsilon can be decreased to reduce the effect
 of noise.
 See Figure X for details.
\end_layout

\begin_layout Standard
Assumption: clusters that are dense will have all members of a similar proximity.
 Assuming the starting Epsilon value locates many points, we consider that
 this is a dense cluster.
 The epsilon value is thereby decreased accordingly, and following neighbours
 are checked for other related points.
 As more points are added, the epsilon value decreases accordingly, thus
 preventing the possibility of locating bridges and bringing these types
 of points into the current cluster.
\end_layout

\begin_layout Subsection
UltraK-Medoids
\end_layout

\begin_layout Standard
Only recalculate the medoid associations of points once per swap iteration.
 At present, the algorithm re-calculates the medoid assignments for all
 points in order to determine the total cost of swapping the system.
 In many situations, this would be grossly inefficient as most points should
 remain associated with the same medoid.
 The only data instances that would change are those which have close to
 dual-medoid membership, given the location of the new, potential medoid.
 Thus, the system instead only swaps the medoid out temporarily with the
 other randomly selected medoid, and then computes the total cost without
 recomputing all distances.
 
\end_layout

\begin_layout Standard
The result is that the time complexity reduces from approximately 
\begin_inset Formula $O(2$
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Results-and-Discussion"

\end_inset

Results and Discussion
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "Sub:Experimental-Details"

\end_inset

Experimental Details
\end_layout

\begin_layout Standard
The experimentation detailed in this section was carried out within the
 Waikato Environment for Knowledge Analysis (WEKA) suite for machine learning.
 This software, developed in the Java programming language, offers a powerful
 testing harness for analysis of various data mining concepts and implementation
s.
 In addition, it offers the ability to extend the suite with additional
 modules, and in the case of the presented work, clustering methods.
 
\end_layout

\begin_layout Standard
Building upon the functionality that already exists within WEKA, several
 existing techniques - specifically, DBSCAN and K-Medoids - were implemented,
 as were the customisations, as detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Customisations"

\end_inset

.
 Once developed, these clustering algorithms were applied the specified
 test data set of forest fire data to determine their effectiveness and
 efficiency.
 
\end_layout

\begin_layout Standard
The tests were performed on the same computing hardware in order to ensure
 objectivity - a recent-model desktop computer with a 2.5GHz Intel Centrino
 dual-core processor, 3GB of RAM, and running the latest stable release
 of the Java Virtual Machine (JVM), version 1.6.
 These experiments examined a range of varying input parameters to determine
 their effect on an algorithm's results, and to determine any changes in
 speed.
 For each different input and clustering technique, 
\end_layout

\begin_layout Standard
Need to run a comparison (on our sample data set) of our two algorithms
 we've implemented, and several others that already exist within WEKA.
 How does their 1) 
\emph on
performance
\emph default
 and 2) 
\emph on
accuracy
\emph default
 stand up to one another? May need to look at how many clusters (on average)
 each method produces and them perform the same function like this (keep
 running until completed)
\end_layout

\begin_layout Standard
and to see if the customisations offer any improvement in speed or results
 produced over their original counterparts
\end_layout

\begin_layout Subsection
Comparison Results
\end_layout

\begin_layout Standard
Discuss what the results were here, so detail the graphs and tables for
 efficiency and numbers etc.
\end_layout

\begin_layout Subparagraph*
Density Based
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeed.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-Performance-Results"

\end_inset

Density-based performance by epsilon
\end_layout

\end_inset


\end_layout

\end_inset

Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DBSCAN-Performance-Results"

\end_inset

 shows a detailed comparison of performance of various density-based clustering
 algorithms, and demonstrates the speed of clustering, based upon a differing
 epsilon distance value.
 The epsilon value is varied to determine its effect upon each technique,
 given that it could be expected that a larger epsilon value would see generally
 larger clusters with greater quantities of members in each cluster.
 
\end_layout

\begin_layout Standard
As detailed in the figure, each of these algorithms operates in a similar
 time complexity.
 What is evident, however, is that generally, as epsilon increases, there
 will be a marginal increase in the amount of time necessary to complete
 the clustering operation.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscanspeedattrib.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-attrib"

\end_inset

Density-based performance by attributes
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dbscannoise.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DBSCAN-performance-noise"

\end_inset

Density-based performance by resulting noise
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoidsspeed.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Partitioning-performance-k"

\end_inset

Partitioning performance by 
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename kmedoidspeedattrib.png
	lyxscale 50
	width 85col%

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Partitioning-performance-attrib"

\end_inset

Partitioning performance by attributes
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Conclusions
\end_layout

\begin_layout Standard
As we detail the results, we should offer a discussion as to what they are,
 and why they are like that.
\end_layout

\begin_layout Standard
Eg draw conclusions here and offer explanations
\end_layout

\begin_layout Section
Issues
\end_layout

\begin_layout Standard
As is to be expected of any data mining procedure, the best results from
 a clustering operation depend entirely upon many different factors.
 These include, but are not limited to the actual data set being processed,
 the clustering technique selected, and the input parameters provided to
 such technique.
 In some situations, a certain algorithm may determine suitable clusters
 within one data set, but may be completely ineffective with another.
 Likewise, using unsuitable input parameters for a given clustering process
 may result in similarly unsuitable cluster assignments for data instances.
 All of these aspects of data mining thereby need to be reviewed before
 being used, and perferably by a person involved who has sufficient domain
 knowledge to make correct descisions that will reveal suitable output.
\end_layout

\begin_layout Standard
Furthermore, as detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Related-Work"

\end_inset

, the most suitable types of data are detailed for the given subsets of
 clustering techniques.
 It should be noted that the results demonstrated in this paper describe
 tests executed within the given testing environment, and only represent
 results associated with the one data set of forest fire information being
 used.
 Refer to Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sub:Experimental-Data-Set"

\end_inset

 for details regarding this test data set.
 As mentioned previously, using a different set of data may, and most likely
 will, reveal different patterns and see a change in processing time complexity.
 That said, the results detailed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "Sec:Results-and-Discussion"

\end_inset

 indicate for the data set under consideration, the DBSCAN and K-Medoids
 algorithms, and their customised counterparts, reveal useful clusters.
 From this, it can be generally inferred that for similar types of data,
 these same algorithms should result in similar types of clusters being
 found and similar durations of processing required.
\end_layout

\begin_layout Standard
In summary, 
\end_layout

\begin_layout Standard
Discuss the results in this section
\end_layout

\begin_layout Standard
Is there sufficient evidence here to show that our algorithms are worthy
 of use?
\end_layout

\begin_layout Standard
Issues - What drawbacks do we see with our various sets of data and algorithms
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Conclusion"

\end_inset

Conclusion
\end_layout

\begin_layout Standard
Overall: better or worse? Able to satisfy or not-possible? It might not
 be given the complexity and time limitations
\end_layout

\begin_layout Standard
summarise topic and close.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "Sec:Future-Work"

\end_inset

Future Work
\end_layout

\begin_layout Standard
Describe potential improvements to the methodologies we've presented within
 this paper.
 Others can extend our work by seeking other ways of improving the semantics
 of the algorithm, rather than attempting to improve its operation itself.
 Given that these algorithms this paper discusses have been in existence
 for <X> years, and that there have been many various papers and theses
 produced regarding these issues, this appears the logical path for improvements
 to take.
 Should any future developers decide to follow through on the same path,
 this paper has logically set out its concepts and ideas, and will provide
 a solid basis for such research.
\end_layout

\begin_layout Standard
As the topic of data mining, and more specifically, clustering, becomes
 more of a part of every-day business and organisational operation, the
 necessity for faster and equally accurate algorithms rises.
 This paper has shown this, and several other applications do exist, and
 it can only be assumed that 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "clustersdk"
options "latex8"

\end_inset


\end_layout

\end_body
\end_document
